<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>【Hadoop学习笔记 -- 在HDFS上运行MapReduce程序】</title>
    <url>/hadoop-02.html</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h3 id="1-测试为分布式案例"><a href="#1-测试为分布式案例" class="headerlink" title="1.测试为分布式案例"></a>1.测试为分布式案例</h3><h4 id="1-在HDFS上运行MapReduce程序"><a href="#1-在HDFS上运行MapReduce程序" class="headerlink" title="1.在HDFS上运行MapReduce程序"></a>1.在HDFS上运行MapReduce程序</h4><h5 id="1-1准备一台客户机"><a href="#1-1准备一台客户机" class="headerlink" title="1.1准备一台客户机"></a>1.1准备一台客户机</h5><h5 id="1-2安装JDK"><a href="#1-2安装JDK" class="headerlink" title="1.2安装JDK"></a>1.2安装JDK</h5><h5 id="1-3配置JDK的环境变量"><a href="#1-3配置JDK的环境变量" class="headerlink" title="1.3配置JDK的环境变量"></a>1.3配置JDK的环境变量</h5><h5 id="1-4安装Hadoop"><a href="#1-4安装Hadoop" class="headerlink" title="1.4安装Hadoop"></a>1.4安装Hadoop</h5><h5 id="1-5配置Hadoop的环境变量"><a href="#1-5配置Hadoop的环境变量" class="headerlink" title="1.5配置Hadoop的环境变量"></a>1.5配置Hadoop的环境变量</h5><a id="more"></a>

<h5 id="1-6配置集群"><a href="#1-6配置集群" class="headerlink" title="1.6配置集群"></a>1.6配置集群</h5><h6 id="配置：hadoop-env-sh"><a href="#配置：hadoop-env-sh" class="headerlink" title="配置：hadoop-env.sh"></a>配置：hadoop-env.sh</h6><ul>
<li>更换JDK环境变量（同笔记01）<h6 id="配置：core-site-xml"><a href="#配置：core-site-xml" class="headerlink" title="配置：core-site.xml"></a>配置：core-site.xml</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- 指定HDFS中NameNode的地址 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;fs.defaultFS&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;hdfs:&#x2F;&#x2F;hadoop101:8020&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">    </span><br><span class="line">    &lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span><br><span class="line">    &lt;!-- 开始搭建时可以暂不考虑 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;data&#x2F;tmp&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt; </span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>
<h6 id="配置：hdfs-site-xml"><a href="#配置：hdfs-site-xml" class="headerlink" title="配置：hdfs-site.xml"></a>配置：hdfs-site.xml</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;configuration&gt;</span><br><span class="line">    &lt;!-- 指定HDFS副本的数量:</span><br><span class="line">        如果有只一台机器，即使副本数配置成3份，也只会备份1个 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;dfs.replication&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;1&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br><span class="line">&lt;&#x2F;configuration&gt;</span><br></pre></td></tr></table></figure>
<h5 id="1-7启动集群"><a href="#1-7启动集群" class="headerlink" title="1.7启动集群"></a>1.7启动集群</h5><h6 id="1-格式化NameNode（第一次启动时格式化，以后就不要总格式化）"><a href="#1-格式化NameNode（第一次启动时格式化，以后就不要总格式化）" class="headerlink" title="1.格式化NameNode（第一次启动时格式化，以后就不要总格式化）"></a>1.格式化NameNode（第一次启动时格式化，以后就不要总格式化）</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hdfs namenode -format</span><br><span class="line">注：该命令在bin目录下</span><br></pre></td></tr></table></figure>
<h6 id="2-启动NameNode"><a href="#2-启动NameNode" class="headerlink" title="2.启动NameNode"></a>2.启动NameNode</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop-daemon.sh start namenode</span><br><span class="line">注：该命令在sbin目录下</span><br></pre></td></tr></table></figure>
<h6 id="3-启动DataNode"><a href="#3-启动DataNode" class="headerlink" title="3.启动DataNode"></a>3.启动DataNode</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop-daemon.sh start datanode</span><br><span class="line">注：该命令在sbin目录下</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h5 id="1-8查看集群"><a href="#1-8查看集群" class="headerlink" title="1.8查看集群"></a>1.8查看集群</h5><h6 id="1-jps命令查看启动进程"><a href="#1-jps命令查看启动进程" class="headerlink" title="1.jps命令查看启动进程"></a>1.jps命令查看启动进程</h6><h6 id="2-查看产生的log日志"><a href="#2-查看产生的log日志" class="headerlink" title="2.查看产生的log日志"></a>2.查看产生的log日志</h6><ul>
<li>cd /hadoop-2.7.2/logs<h6 id="3-WEB端查看HDFS文件系统"><a href="#3-WEB端查看HDFS文件系统" class="headerlink" title="3.WEB端查看HDFS文件系统"></a>3.WEB端查看HDFS文件系统</h6></li>
<li>URL： 主机地址:50070</li>
</ul>
<h5 id="1-9操作集群"><a href="#1-9操作集群" class="headerlink" title="1.9操作集群"></a>1.9操作集群</h5><h6 id="1-在HDFS文件系统上新建一个input文件夹"><a href="#1-在HDFS文件系统上新建一个input文件夹" class="headerlink" title="1.在HDFS文件系统上新建一个input文件夹"></a>1.在HDFS文件系统上新建一个input文件夹</h6><ul>
<li>hadoop fs 和 bin/hdfs dfs 通用</li>
<li><strong>HDFS默认的根目录是：</strong>/</li>
<li>hadoop fs -mkdir -p /user/giraffey375/input (-p表示多级目录)</li>
<li><strong>在WEB查看创建结果</strong></li>
<li><strong>在Shell端查看:</strong><ul>
<li>hadoop fs -ls -R /</li>
<li>或：hadoop fs -lsr /<h6 id="2-上传测试文件到HDFS"><a href="#2-上传测试文件到HDFS" class="headerlink" title="2.上传测试文件到HDFS"></a>2.上传测试文件到HDFS</h6></li>
</ul>
</li>
<li>hadoop fs -put wcinput/wc.input(测试文件) /user/giraffey375/input(目标文件位置)<h6 id="3-在HDFS上运行mapreduce程序"><a href="#3-在HDFS上运行mapreduce程序" class="headerlink" title="3.在HDFS上运行mapreduce程序"></a>3.在HDFS上运行mapreduce程序</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop jar share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-2.7.2 jar wordcount(案例名称) &#x2F;user&#x2F;giraffey375&#x2F;input &#x2F;user&#x2F;giraffey375&#x2F;output</span><br></pre></td></tr></table></figure>
<h6 id="4-查看执行结果"><a href="#4-查看执行结果" class="headerlink" title="4.查看执行结果"></a>4.查看执行结果</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop fs -cat &#x2F;user&#x2F;giraffey375&#x2F;output&#x2F;p*</span><br></pre></td></tr></table></figure>
<h6 id="5-删除输出文件夹：否则下次就不能运行了"><a href="#5-删除输出文件夹：否则下次就不能运行了" class="headerlink" title="5.删除输出文件夹：否则下次就不能运行了"></a>5.删除输出文件夹：否则下次就不能运行了</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop fs -rmr &#x2F;user&#x2F;giraffey375&#x2F;output</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="注："><a href="#注：" class="headerlink" title="注："></a>注：</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在第二次启动时，会有文件残留。</span><br><span class="line">因为 hdfs namenode -format 产生了一个新的Namenode文件(id)，就不认识之前的datanode了，所以导致集群不能正常启动。</span><br><span class="line">解决办法：在格式化之前，删除datanode中的信息（默认在&#x2F;tmp,如果配置了该目录，就去到该目录(hadoop-2.7.2&#x2F;data&#x2F;tmp)删除）</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>【Hadoop学习笔记 -- 在Yarn上运行MapReduce程序】</title>
    <url>/hadoop-03.html</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h3 id="1-测试为分布式案例"><a href="#1-测试为分布式案例" class="headerlink" title="1.测试为分布式案例"></a>1.测试为分布式案例</h3><h4 id="1-在YARN上运行MapReduce程序（分析）"><a href="#1-在YARN上运行MapReduce程序（分析）" class="headerlink" title="1.在YARN上运行MapReduce程序（分析）"></a>1.在YARN上运行MapReduce程序（分析）</h4><h5 id="1-1准备一台客户机"><a href="#1-1准备一台客户机" class="headerlink" title="1.1准备一台客户机"></a>1.1准备一台客户机</h5><h5 id="1-2安装JDK"><a href="#1-2安装JDK" class="headerlink" title="1.2安装JDK"></a>1.2安装JDK</h5><h5 id="1-3配置JDK的环境变量"><a href="#1-3配置JDK的环境变量" class="headerlink" title="1.3配置JDK的环境变量"></a>1.3配置JDK的环境变量</h5><h5 id="1-4安装Hadoop"><a href="#1-4安装Hadoop" class="headerlink" title="1.4安装Hadoop"></a>1.4安装Hadoop</h5><h5 id="1-5配置Hadoop的环境变量"><a href="#1-5配置Hadoop的环境变量" class="headerlink" title="1.5配置Hadoop的环境变量"></a>1.5配置Hadoop的环境变量</h5><h5 id="1-6配置集群在YARN上运行"><a href="#1-6配置集群在YARN上运行" class="headerlink" title="1.6配置集群在YARN上运行"></a>1.6配置集群在YARN上运行</h5><h5 id="1-7启用集群，增删改查测试"><a href="#1-7启用集群，增删改查测试" class="headerlink" title="1.7启用集群，增删改查测试"></a>1.7启用集群，增删改查测试</h5><h5 id="1-8在YARN上运行wordcount案例"><a href="#1-8在YARN上运行wordcount案例" class="headerlink" title="1.8在YARN上运行wordcount案例"></a>1.8在YARN上运行wordcount案例</h5><a id="more"></a>

<hr>
<h4 id="2-执行步骤"><a href="#2-执行步骤" class="headerlink" title="2.执行步骤"></a>2.执行步骤</h4><h5 id="1-配置集群"><a href="#1-配置集群" class="headerlink" title="1.配置集群"></a>1.配置集群</h5><h6 id="a-配置yarn-env-sh"><a href="#a-配置yarn-env-sh" class="headerlink" title="a.配置yarn-env.sh"></a>a.配置yarn-env.sh</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#配置JAVA_HOME</span><br></pre></td></tr></table></figure>
<h6 id="b-配置yarn-site-xml"><a href="#b-配置yarn-site-xml" class="headerlink" title="b.配置yarn-site.xml"></a>b.配置yarn-site.xml</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;!-- Reduce获取数据的方式:NodeManager --&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- 指定YARN的ResourceManager地址 --&gt;</span><br></pre></td></tr></table></figure>
<h6 id="c-配置mapred-env-sh"><a href="#c-配置mapred-env-sh" class="headerlink" title="c.配置mapred-env.sh"></a>c.配置mapred-env.sh</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#配置JAVA_HOME</span><br></pre></td></tr></table></figure>
<h6 id="d-配置mapred-site-xml-对mapred-site-xml-template重新命名为mapred-site-xml"><a href="#d-配置mapred-site-xml-对mapred-site-xml-template重新命名为mapred-site-xml" class="headerlink" title="d.配置mapred-site.xml(对mapred-site.xml.template重新命名为mapred-site.xml)"></a>d.配置mapred-site.xml(对mapred-site.xml.template重新命名为mapred-site.xml)</h6><h6 id="e-打印JAVA-HOME路径"><a href="#e-打印JAVA-HOME路径" class="headerlink" title="e.打印JAVA_HOME路径"></a>e.打印JAVA_HOME路径</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">echo $JAVA_HOME</span><br></pre></td></tr></table></figure>

<h5 id="2-启动集群"><a href="#2-启动集群" class="headerlink" title="2.启动集群"></a>2.启动集群</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 启动namenode和datanode</span><br><span class="line">sbin&#x2F;hadoop-daemon.sh start namenode</span><br><span class="line">sbin&#x2F;hadoop-daemon.sh start datanode</span><br><span class="line"></span><br><span class="line"># 启动资源管理器和节点管理器</span><br><span class="line">yarn-daemon.sh start resourcemanager</span><br><span class="line">yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure>

<h5 id="3-集群操作"><a href="#3-集群操作" class="headerlink" title="3.集群操作"></a>3.集群操作</h5><h6 id="1-WEB端查YARN"><a href="#1-WEB端查YARN" class="headerlink" title="1.WEB端查YARN"></a>1.WEB端查YARN</h6><ul>
<li>URL： hadoop101(主机名):8088/cluster</li>
</ul>
<h6 id="2-删除文件系统上的output文件"><a href="#2-删除文件系统上的output文件" class="headerlink" title="2.删除文件系统上的output文件"></a>2.删除文件系统上的output文件</h6><h6 id="3-执行MapReduce程序"><a href="#3-执行MapReduce程序" class="headerlink" title="3.执行MapReduce程序"></a>3.执行MapReduce程序</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;hadoop jar share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-2.7.2 jar wordcount(案例名称) &#x2F;user&#x2F;giraffey375&#x2F;input &#x2F;user&#x2F;giraffey375&#x2F;output</span><br></pre></td></tr></table></figure>
<h6 id="4-删除output文件"><a href="#4-删除output文件" class="headerlink" title="4.删除output文件"></a>4.删除output文件</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop fs -rm -r &#x2F;user&#x2F;giraffey375&#x2F;output</span><br></pre></td></tr></table></figure>
<h6 id="5-再次执行"><a href="#5-再次执行" class="headerlink" title="5.再次执行"></a>5.再次执行</h6>]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>【Hadoop学习笔记 -- 搭建Hadoop环境】</title>
    <url>/hadoop-01.html</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><hr>
<h4 id="名词解释："><a href="#名词解释：" class="headerlink" title="名词解释："></a>名词解释：</h4><p>Hadoop:</p>
<ul>
<li>HDFS:<ul>
<li>Namenode:管理目录文件，相当于书的目录；</li>
<li>Datanode:管理数据存储具体的数据,相当于具体的书；</li>
<li>Secondarynamenode:是辅助NameNode工作的;</li>
</ul>
</li>
<li>YARN：资源调度管理器<ul>
<li>ResourceManager：总的管理器</li>
<li>NodeManager：具体节点的管理器</li>
</ul>
</li>
<li>MapReduce:<ul>
<li>Map：并行的将文件读入</li>
<li>Reduce：将并行过来的文件合并成单一出口</li>
</ul>
</li>
</ul>
<a id="more"></a>

<hr>
<h4 id="1-克隆主机"><a href="#1-克隆主机" class="headerlink" title="1.克隆主机"></a>1.克隆主机</h4><hr>
<h4 id="2-关闭防火墙"><a href="#2-关闭防火墙" class="headerlink" title="2.关闭防火墙"></a>2.关闭防火墙</h4><ul>
<li>chkconfig</li>
<li>chkconfig iptables –list</li>
<li><strong>注：Centos 7以下版本防火墙为iptables</strong></li>
</ul>
<hr>
<h4 id="3-修改主机配置"><a href="#3-修改主机配置" class="headerlink" title="3.修改主机配置"></a>3.修改主机配置</h4><h6 id="管理员授权"><a href="#管理员授权" class="headerlink" title="管理员授权"></a>管理员授权</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sudo -i&#x2F;su root</span><br></pre></td></tr></table></figure>
<h6 id="修改xxx"><a href="#修改xxx" class="headerlink" title="修改xxx"></a>修改xxx</h6><ul>
<li>vi /etc/udev/rules.d/70-persistent-net.rules</li>
<li>键盘操作：dd (删除整行)</li>
<li>键盘操作：shift+$(快速回到行尾)</li>
<li>复制物理地址</li>
</ul>
<h6 id="修改IP地址、网关、BOOTPROTO、ONBOOT、DNS1"><a href="#修改IP地址、网关、BOOTPROTO、ONBOOT、DNS1" class="headerlink" title="修改IP地址、网关、BOOTPROTO、ONBOOT、DNS1"></a>修改IP地址、网关、BOOTPROTO、ONBOOT、DNS1</h6><ul>
<li>vi /etc/sysconfig/network-scripts/ifcfg-eth0<h6 id="修改主机名称"><a href="#修改主机名称" class="headerlink" title="修改主机名称"></a>修改主机名称</h6></li>
<li>vi /etc/sysconfig/network</li>
<li>配置xxxx<ul>
<li>vi /etc/hosts<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">192.168.x.x(IP地址) 主机名1</span><br><span class="line">...</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<hr>
<h5 id="4-重启主机"><a href="#4-重启主机" class="headerlink" title="4.重启主机"></a>4.重启主机</h5><ul>
<li>sync</li>
<li>reboot </li>
</ul>
<hr>
<h5 id="5-重启后测试"><a href="#5-重启后测试" class="headerlink" title="5.重启后测试"></a>5.重启后测试</h5><ul>
<li>1.测试ip<ul>
<li>ipconfig</li>
</ul>
</li>
<li>2.查看主机名称<ul>
<li>hostname</li>
</ul>
</li>
<li>3.ping通主机</li>
</ul>
<hr>
<h5 id="6-管理员和普通用户"><a href="#6-管理员和普通用户" class="headerlink" title="6.管理员和普通用户"></a>6.管理员和普通用户</h5><h6 id="普通用户提权"><a href="#普通用户提权" class="headerlink" title="普通用户提权"></a>普通用户提权</h6><ul>
<li>vi /etc/sudoers</li>
</ul>
<h6 id="切换用户"><a href="#切换用户" class="headerlink" title="切换用户"></a>切换用户</h6><ul>
<li>su 用户名</li>
</ul>
<h6 id="普通用户使用管理员权限"><a href="#普通用户使用管理员权限" class="headerlink" title="普通用户使用管理员权限"></a>普通用户使用管理员权限</h6><ul>
<li>sudo xxxx(命令)</li>
<li><h6 id="更改文件-夹-拥有者和组"><a href="#更改文件-夹-拥有者和组" class="headerlink" title="更改文件(夹)拥有者和组"></a>更改文件(夹)拥有者和组</h6></li>
<li>sudo chown 用户名:组名 文件(夹)1 文件(夹)2</li>
<li>chown -R ??????</li>
</ul>
<hr>
<h4 id="7-安装环境"><a href="#7-安装环境" class="headerlink" title="7.安装环境"></a>7.安装环境</h4><h5 id="配置JDK"><a href="#配置JDK" class="headerlink" title="配置JDK"></a>配置JDK</h5><h6 id="查看有没有jdk"><a href="#查看有没有jdk" class="headerlink" title="查看有没有jdk"></a>查看有没有jdk</h6><ul>
<li>rpm -qa | grep java<h6 id="去掉jdk"><a href="#去掉jdk" class="headerlink" title="去掉jdk"></a>去掉jdk</h6></li>
<li>rpm -e 文件包名</li>
</ul>
<h6 id="解压tar文件到指定目录下"><a href="#解压tar文件到指定目录下" class="headerlink" title="解压tar文件到指定目录下"></a>解压tar文件到指定目录下</h6><ul>
<li>tar -zxvf xxx.gz -C /指定的文件夹位置</li>
<li>将JDK解压缩到/opt/module目录下</li>
</ul>
<h6 id="配置环境变量"><a href="#配置环境变量" class="headerlink" title="配置环境变量"></a>配置环境变量</h6><ul>
<li>sudo vi /etc/profile<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#JAVA_HOME</span><br><span class="line">export JAVA_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;jdk路径</span><br><span class="line">export PATH&#x3D;$PATH:$JAVA_HOME&#x2F;bin</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h5 id="配置Hadoop环境变量"><a href="#配置Hadoop环境变量" class="headerlink" title="配置Hadoop环境变量"></a>配置Hadoop环境变量</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#HADOOP_HOME</span><br><span class="line">export HADOOP_HOME&#x3D;&#x2F;opt&#x2F;module&#x2F;hadoop路径</span><br><span class="line">export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;bin</span><br><span class="line">export PATH&#x3D;$PATH:$HADOOP_HOME&#x2F;sbin</span><br></pre></td></tr></table></figure>

<h6 id="使配置生效"><a href="#使配置生效" class="headerlink" title="使配置生效"></a>使配置生效</h6><ul>
<li>source /etc/hadoop/profile</li>
</ul>
<h6 id="配置-etc-hadoop-env-sh"><a href="#配置-etc-hadoop-env-sh" class="headerlink" title="配置 /etc/hadoop-env.sh"></a>配置 /etc/hadoop-env.sh</h6><ul>
<li>配置JDK<ul>
<li>将之前的JDK环境变量重新配置</li>
<li>之前的${JAVA_HOME}在远程系统调用时可能会拿不到JDK的变量值</li>
</ul>
</li>
</ul>
<hr>
<h4 id="8-测试Hadoopd的官方案例"><a href="#8-测试Hadoopd的官方案例" class="headerlink" title="8.测试Hadoopd的官方案例"></a>8.测试Hadoopd的官方案例</h4><h5 id="案例1–grep-查找众多文件中符合正则表达式的数据"><a href="#案例1–grep-查找众多文件中符合正则表达式的数据" class="headerlink" title="案例1–grep:查找众多文件中符合正则表达式的数据"></a>案例1–grep:查找众多文件中符合正则表达式的数据</h5><h6 id="在hadoop-2-x-x文件夹下新建input文件夹"><a href="#在hadoop-2-x-x文件夹下新建input文件夹" class="headerlink" title="在hadoop-2.x.x文件夹下新建input文件夹"></a>在hadoop-2.x.x文件夹下新建input文件夹</h6><h6 id="将-etc-hadoop-xml文件拷贝到在input文件夹"><a href="#将-etc-hadoop-xml文件拷贝到在input文件夹" class="headerlink" title="将/etc/hadoop/*.xml文件拷贝到在input文件夹"></a>将/etc/hadoop/*.xml文件拷贝到在input文件夹</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp &#x2F;etc&#x2F;hadoop&#x2F;*.xml &#x2F;input</span><br></pre></td></tr></table></figure>
<h6 id="在hadoop文件夹下，执行share中的simple案例"><a href="#在hadoop文件夹下，执行share中的simple案例" class="headerlink" title="在hadoop文件夹下，执行share中的simple案例"></a>在hadoop文件夹下，执行share中的simple案例</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop jar share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-2.7.2 jar grep(案例名称) input(输入文件夹) output(查询结果文件夹，并且不允许存在output这个文件夹) &#39;dfs[a-z.]+&#39; (正则校验：以dfs开头，[]表示[]中内容为可选，+表示还可以有后续的)</span><br></pre></td></tr></table></figure>
<ul>
<li>输出文件在output文件夹中的文件中<h5 id="案例2–wordcount-统计众多文件中每个文件中单词的个数"><a href="#案例2–wordcount-统计众多文件中每个文件中单词的个数" class="headerlink" title="案例2–wordcount:统计众多文件中每个文件中单词的个数"></a>案例2–wordcount:统计众多文件中每个文件中单词的个数</h5><h6 id="在hadoop-2-x-x文件夹下新建wcinput文件夹"><a href="#在hadoop-2-x-x文件夹下新建wcinput文件夹" class="headerlink" title="在hadoop-2.x.x文件夹下新建wcinput文件夹"></a>在hadoop-2.x.x文件夹下新建wcinput文件夹</h6><h6 id="在wcinput文件夹下新建wc-input文件"><a href="#在wcinput文件夹下新建wc-input文件" class="headerlink" title="在wcinput文件夹下新建wc.input文件"></a>在wcinput文件夹下新建wc.input文件</h6><h6 id="在wc-input文件中存放一些测试使用的单词数据"><a href="#在wc-input文件中存放一些测试使用的单词数据" class="headerlink" title="在wc.input文件中存放一些测试使用的单词数据"></a>在wc.input文件中存放一些测试使用的单词数据</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop jar share&#x2F;hadoop&#x2F;mapreduce&#x2F;hadoop-mapreduce-examples-2.7.2 jar wordcount(案例名称) wcinput(要统计的文件夹) wcoutput(查询结果文件夹)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<hr>
<h3 id="其他："><a href="#其他：" class="headerlink" title="其他："></a>其他：</h3><h5 id="VIM编辑器常用快捷键"><a href="#VIM编辑器常用快捷键" class="headerlink" title="VIM编辑器常用快捷键"></a>VIM编辑器常用快捷键</h5><ul>
<li>键盘操作：Shift + G  (文件最后一行左下角)</li>
<li>键盘操作: o  ()</li>
<li>键盘操作：dd  （删除整行）</li>
<li>‘#’  （注释）</li>
<li>键盘操作：yyt  （复制当前行到下一行） </li>
<li>退回到最旧的操作：</li>
</ul>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>【JAVA补血大法】</title>
    <url>/java-day00.html</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h3 id="准备工作"><a href="#准备工作" class="headerlink" title="准备工作"></a>准备工作</h3><pre><code></code></pre>]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>java</tag>
        <tag>jvm</tag>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title>【Hadoop学习笔记 -- 完全分布式搭建】</title>
    <url>/hadoop-05.html</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><strong>前言</strong>: 之前的笔记中记录了Hadoop伪分布式的搭建，但是在实际中，伪分布式是远远不能够满足需求的。</p>
<a id="more"></a>

<h4 id="1-准备虚拟机搭建环境-CentOS6"><a href="#1-准备虚拟机搭建环境-CentOS6" class="headerlink" title="1.准备虚拟机搭建环境(CentOS6)"></a>1.准备虚拟机搭建环境(CentOS6)</h4><h5 id="1-准备3台客户机（关闭防火墙、静态IP、设置主机名称）"><a href="#1-准备3台客户机（关闭防火墙、静态IP、设置主机名称）" class="headerlink" title="1.准备3台客户机（关闭防火墙、静态IP、设置主机名称）"></a>1.准备3台客户机（关闭防火墙、静态IP、设置主机名称）</h5><h6 id="1-修改IP相关配置"><a href="#1-修改IP相关配置" class="headerlink" title="1.修改IP相关配置"></a>1.修改IP相关配置</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">su root</span><br><span class="line">vi &#x2F;etc&#x2F;udev&#x2F;rules.d&#x2F;70-persistent-net.rules</span><br><span class="line">Shift+$ --&gt; 到末尾</span><br><span class="line">修改NAME</span><br><span class="line">复制物理地址</span><br><span class="line"></span><br><span class="line">vi &#x2F;etc&#x2F;sysconfig&#x2F;network-scripts&#x2F;ifcfg-eth0</span><br><span class="line">粘贴物理地址</span><br><span class="line">修改IP地址</span><br></pre></td></tr></table></figure>
<p><img src="pic.owlhy.com/blog/20200602223731.png" alt="image"></p>
<h6 id="2-修改主机名"><a href="#2-修改主机名" class="headerlink" title="2.修改主机名"></a>2.修改主机名</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hostname</span><br><span class="line">vi &#x2F;etc&#x2F;sysconfig&#x2F;network</span><br></pre></td></tr></table></figure>
<p><img src="pic.owlhy.com/blog/20200602223756.png" alt="image"></p>
<h6 id="3-重启"><a href="#3-重启" class="headerlink" title="3.重启"></a>3.重启</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">reboot</span><br></pre></td></tr></table></figure>
<h6 id="4-PING外网，测试网络连通性"><a href="#4-PING外网，测试网络连通性" class="headerlink" title="4.PING外网，测试网络连通性"></a>4.PING外网，测试网络连通性</h6><hr>
<h5 id="2-安装JDK"><a href="#2-安装JDK" class="headerlink" title="2.安装JDK"></a>2.安装JDK</h5><h6 id="0-切换到root用户"><a href="#0-切换到root用户" class="headerlink" title="0.切换到root用户"></a>0.切换到root用户</h6><h6 id="1-清空-opt目录"><a href="#1-清空-opt目录" class="headerlink" title="1.清空/opt目录"></a>1.清空/opt目录</h6><h6 id="2-创建文件夹software和module"><a href="#2-创建文件夹software和module" class="headerlink" title="2.创建文件夹software和module"></a>2.创建文件夹software和module</h6><h6 id="3-授权"><a href="#3-授权" class="headerlink" title="3.授权"></a>3.授权</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">chown giraffey:giraffey module software</span><br></pre></td></tr></table></figure>
<h6 id="4-切换到普通（giraffey）用户"><a href="#4-切换到普通（giraffey）用户" class="headerlink" title="4.切换到普通（giraffey）用户"></a>4.切换到普通（giraffey）用户</h6><h6 id="5-上传安装包"><a href="#5-上传安装包" class="headerlink" title="5.上传安装包"></a>5.上传安装包</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">主机之间可以使用scp命令进行拷贝</span><br><span class="line">scp -r（递归） 需要拷贝的文件 用户@主机名或主机地址:&#x2F;opt</span><br></pre></td></tr></table></figure>

<h5 id="3-配置JDK环境变量"><a href="#3-配置JDK环境变量" class="headerlink" title="3.配置JDK环境变量"></a>3.配置JDK环境变量</h5><h5 id="4-安装Hadoop"><a href="#4-安装Hadoop" class="headerlink" title="4.安装Hadoop"></a>4.安装Hadoop</h5><h5 id="5-配置Hadoop环境变量"><a href="#5-配置Hadoop环境变量" class="headerlink" title="5.配置Hadoop环境变量"></a>5.配置Hadoop环境变量</h5><h5 id="6-安装ssh"><a href="#6-安装ssh" class="headerlink" title="6.安装ssh"></a>6.安装ssh</h5><h6 id="配置ssh免密登录"><a href="#配置ssh免密登录" class="headerlink" title="配置ssh免密登录"></a>配置ssh免密登录</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.进入到想要免密登录的用户的家目录下</span><br><span class="line">cd ~</span><br><span class="line">ls -la</span><br><span class="line">cd .ssh</span><br><span class="line">2.生成秘钥对</span><br><span class="line">ssh-keygen -t rsa</span><br><span class="line">连敲三个回车</span><br><span class="line"></span><br><span class="line">3.拷贝公钥</span><br><span class="line">ssh-copy-id 拷贝到的主机名或主机地址</span><br><span class="line">eg: ssh-copy-id 192.168.1.1</span><br><span class="line">注：本主机也要拷贝一次</span><br></pre></td></tr></table></figure>
<h5 id="7-rsync远程同步工具，主要用于备份和镜像，具有速度快、避免复制相同内容和支持符号链接的优点"><a href="#7-rsync远程同步工具，主要用于备份和镜像，具有速度快、避免复制相同内容和支持符号链接的优点" class="headerlink" title="7.rsync远程同步工具，主要用于备份和镜像，具有速度快、避免复制相同内容和支持符号链接的优点"></a>7.rsync远程同步工具，主要用于备份和镜像，具有速度快、避免复制相同内容和支持符号链接的优点</h5><p><strong>基本语法：</strong><br>同步时，会替换/覆盖之前已有的文件<br><img src="pic.owlhy.com/blog/20200602223855.png" alt="image"></p>
<h5 id="8-编写集群分发的脚本xsync"><a href="#8-编写集群分发的脚本xsync" class="headerlink" title="8.编写集群分发的脚本xsync"></a>8.编写集群分发的脚本xsync</h5><h6 id="1-循环复制文件到所有节点的相同目录下"><a href="#1-循环复制文件到所有节点的相同目录下" class="headerlink" title="1.循环复制文件到所有节点的相同目录下"></a>1.循环复制文件到所有节点的相同目录下</h6><p><img src="pic.owlhy.com/blog/20200602223915.png" alt="image"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;usr&#x2F;local&#x2F;bin</span><br><span class="line">touch xsync</span><br></pre></td></tr></table></figure>
<p><img src="pic.owlhy.com/blog/20200602223948.png" alt="image"></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">修改脚本权限</span><br><span class="line">chmod 777 xsync</span><br></pre></td></tr></table></figure>

<h5 id="8-编写分发脚本xcall"><a href="#8-编写分发脚本xcall" class="headerlink" title="8.编写分发脚本xcall"></a>8.编写分发脚本xcall</h5><p><img src="pic.owlhy.com/blog/20200602224005.png" alt="image"></p>
<h4 id="注：集群中的用户名密码最好统一，否则再编写脚本时，贼鸡儿麻烦！"><a href="#注：集群中的用户名密码最好统一，否则再编写脚本时，贼鸡儿麻烦！" class="headerlink" title="注：集群中的用户名密码最好统一，否则再编写脚本时，贼鸡儿麻烦！"></a>注：集群中的用户名密码最好统一，否则再编写脚本时，贼鸡儿麻烦！</h4>]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>【Maven插件Assembly进行打包】</title>
    <url>/maven-assembly.html</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="实现功能"><a href="#实现功能" class="headerlink" title="实现功能"></a>实现功能</h2><ul>
<li>将Maven父子工程下的工程分别打包和整体打包</li>
<li>不将配置打到子包中</li>
</ul>
<hr>
<a id="more"></a>

<h2 id="assembly-xml"><a href="#assembly-xml" class="headerlink" title="assembly.xml"></a>assembly.xml</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">assembly</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">id</span>&gt;</span>bin<span class="tag">&lt;/<span class="name">id</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">formats</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--支持 zip，tar，tar.gz，tar.bz2，jar，dir，war 等 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">format</span>&gt;</span>tar.gz<span class="tag">&lt;/<span class="name">format</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">formats</span>&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 指定是否包含打包层目录（比如finalName是output，当值为true，所有文件被放在output目录下，否则直接放在包的根目录下) --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">includeBaseDirectory</span>&gt;</span>true<span class="tag">&lt;/<span class="name">includeBaseDirectory</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">dependencySets</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">dependencySet</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- 是否把本项目添加到依赖文件夹下 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">useProjectArtifact</span>&gt;</span>true<span class="tag">&lt;/<span class="name">useProjectArtifact</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">outputDirectory</span>&gt;</span>lib<span class="tag">&lt;/<span class="name">outputDirectory</span>&gt;</span></span><br><span class="line">            <span class="comment">&lt;!-- 将scope为runtime的依赖包打包 --&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">scope</span>&gt;</span>runtime<span class="tag">&lt;/<span class="name">scope</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">dependencySet</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">dependencySets</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">fileSets</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- /conf --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">fileSet</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">directory</span>&gt;</span>$&#123;project.basedir&#125;/src/main/resources<span class="tag">&lt;/<span class="name">directory</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">outputDirectory</span>&gt;</span>/conf<span class="tag">&lt;/<span class="name">outputDirectory</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">includes</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">include</span>&gt;</span>**/*.xml<span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">include</span>&gt;</span>**/*.yml<span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">include</span>&gt;</span>**/*.properties<span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">include</span>&gt;</span>**/WEB-INF/*.wsdd<span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">includes</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">fileMode</span>&gt;</span>0644<span class="tag">&lt;/<span class="name">fileMode</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">fileSet</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- /conf/elasticsearch --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">fileSet</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">directory</span>&gt;</span>$&#123;project.basedir&#125;/src/main/resources/elasticsearch<span class="tag">&lt;/<span class="name">directory</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">outputDirectory</span>&gt;</span>/conf/elasticsearch<span class="tag">&lt;/<span class="name">outputDirectory</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">includes</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">include</span>&gt;</span>**/*.json<span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">includes</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">fileSet</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- /conf/template --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">fileSet</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">directory</span>&gt;</span>$&#123;project.basedir&#125;/src/main/resources/template<span class="tag">&lt;/<span class="name">directory</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">outputDirectory</span>&gt;</span>/conf/template<span class="tag">&lt;/<span class="name">outputDirectory</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">includes</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">include</span>&gt;</span>**/*.vm<span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">includes</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">fileSet</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- lib --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">fileSet</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">directory</span>&gt;</span>$&#123;project.basedir&#125;/lib<span class="tag">&lt;/<span class="name">directory</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">outputDirectory</span>&gt;</span>/lib<span class="tag">&lt;/<span class="name">outputDirectory</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">includes</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">include</span>&gt;</span>**/*.jar<span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">includes</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">fileSet</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 执行脚本 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">fileSet</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">fileMode</span>&gt;</span>0755<span class="tag">&lt;/<span class="name">fileMode</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">directory</span>&gt;</span>$&#123;project.basedir&#125;/src/main/scripts<span class="tag">&lt;/<span class="name">directory</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">outputDirectory</span>&gt;</span>/<span class="tag">&lt;/<span class="name">outputDirectory</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">includes</span>&gt;</span></span><br><span class="line">                <span class="tag">&lt;<span class="name">include</span>&gt;</span>**/*.sh<span class="tag">&lt;/<span class="name">include</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;/<span class="name">includes</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">fileSet</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">fileSets</span>&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="tag">&lt;/<span class="name">assembly</span>&gt;</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>java</category>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot</tag>
        <tag>maven</tag>
      </tags>
  </entry>
  <entry>
    <title>【LINUX日常命令】</title>
    <url>/linux-base.html</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="实现功能"><a href="#实现功能" class="headerlink" title="实现功能"></a>实现功能</h2><ul>
<li>日常LINUX命令<ul>
<li>查看命令</li>
<li>连接数、打包</li>
</ul>
</li>
</ul>
<hr>
<a id="more"></a>
<h4 id="日常操作"><a href="#日常操作" class="headerlink" title="日常操作"></a>日常操作</h4><h5 id="查看命令"><a href="#查看命令" class="headerlink" title="查看命令"></a>查看命令</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">ssh root@127.0.0.1</span><br><span class="line">scp -r root@10.1.192.190:&#x2F;home&#x2F;itsm-web .&#x2F;          # 拷贝文件（A -&gt; B）</span><br><span class="line">hostname                            # 查看主机名</span><br><span class="line">hostnamectl                         # 查看主机信息</span><br><span class="line">hostnamectl set-hostname giraffey   # 改系统名程</span><br><span class="line">vim &#x2F;etc&#x2F;passwd                     # 修改用户权限</span><br><span class="line">wget http:&#x2F;&#x2F;localhost:8080          # 查看linux中看能否访问tomcat</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">ll                              # 统计数量</span><br><span class="line">                                # -h 查看文件的大小</span><br><span class="line">ls -ltr                         # 将长列表格式按文件或目录的修改时间 倒序地 列出文件和目录</span><br><span class="line"></span><br><span class="line">pwdx &lt;pid&gt;                      # 根据进程号查找程序所在目录</span><br><span class="line">ll &#x2F;proc&#x2F;进程号&#x2F;cwd             # 根据进程号查找程序所在目录</span><br><span class="line">tail -50f xxx.log               # 查看日志</span><br><span class="line">ps aux | grep nginx             # 查看进程</span><br><span class="line">grep -C 5 关键字 xxx            # 查看指定文件中是否包含xxx</span><br><span class="line">find -name mysql                # 通过名字查找关键字为mysql</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">top                             # 查看进程</span><br><span class="line">                                # 按【C】键，查看运行地址</span><br><span class="line">htop                            # 查看进程（插件）</span><br><span class="line">du -sh xxxFile                  # 查看文件大小</span><br><span class="line">sh -x                           # 打断点、调试</span><br><span class="line">echo 1 &gt; &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;drop_caches</span><br><span class="line">watch &quot;ls -l |grep temp&quot;</span><br><span class="line">grep -r RESMClient | more       # 查询包中包含RESMClient的</span><br><span class="line">netstat -anplt | grep &lt;pid&gt;     # 查看监听端口</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">free -h                         # 查看内存情况</span><br><span class="line">fdisk -l                        # 查看硬盘情况</span><br><span class="line">history                         # 查看历史命令操作</span><br><span class="line">mount                           # 查看所有磁盘挂载情况</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">lsof -i :80</span><br><span class="line">lsof -i tcp:22</span><br><span class="line">ps -ef|grep nginx</span><br><span class="line">ps aux|grep ngigx</span><br><span class="line">sudo find &#x2F; -name nginx*</span><br><span class="line">netstat -tunlp                  # -t (tcp) 仅显示tcp相关选项        -u (udp) 仅显示udp相关选项</span><br><span class="line">                                # -n 拒绝显示列名，能显示数字的全部转化为数字   </span><br><span class="line">                                # -l 仅显示出在listen(监听）的服务状态</span><br><span class="line">                                # -p 显示潜力相关链接的程序名</span><br><span class="line"></span><br><span class="line">cat &#x2F;proc&#x2F;meminfo               # 列出了所有你想了解的内存的使用情况</span><br><span class="line">cat &#x2F;proc&#x2F;&lt;pid&gt;&#x2F;statm</span><br><span class="line">cat &#x2F;proc&#x2F;&lt;pid&gt;&#x2F;status</span><br><span class="line"></span><br><span class="line">find .|xargs grep -rin &#39;2020-05-19 05:41&#39; error.log | more  # 根据关键词查日志</span><br></pre></td></tr></table></figure>


<hr>
<h5 id="查看连接数"><a href="#查看连接数" class="headerlink" title="查看连接数"></a>查看连接数</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 查看系统tcp连接中各个状态的连接数</span><br><span class="line">netstat -an | awk &#39;&#x2F;^tcp&#x2F; &#123;++S[$NF]&#125; END &#123;for(a in S) print a, S[a]&#125;&#39;</span><br><span class="line"></span><br><span class="line"># 查看和本机80端口建立连接并状态在established的所有ip</span><br><span class="line">netstat -an |grep 80 |grep ESTA |awk &#39;&#123;print$5 &quot;\n&quot;&#125;&#39; |awk &#39;BEGIN &#123;FS&#x3D;&quot;:&quot;&#125; &#123;print $1 &quot;\n&quot;&#125;&#39; |sort |uniq</span><br><span class="line"></span><br><span class="line"># 输出每个ip的连接数，以及总的各个状态的连接数</span><br><span class="line">netstat -n | awk &#39;&#x2F;^tcp&#x2F; &#123;n&#x3D;split($(NF-1),array,&quot;:&quot;);if(n&lt;&#x3D;2)++S[array[(1)]];else++S[array[(4)]];++s[$NF];++N&#125; END &#123;for(a in S)&#123;printf(&quot;%-20s %s\n&quot;, a, S[a]);++I&#125;printf(&quot;%-20s %s\n&quot;,&quot;TOTAL_IP&quot;,I);for(a in s) printf(&quot;%-20s %s\n&quot;,a, s[a]);printf(&quot;%-20s %s\n&quot;,&quot;TOTAL_LINK&quot;,N);&#125;&#39;</span><br></pre></td></tr></table></figure>

<hr>
<h6 id="查看系统中已经安装的软件"><a href="#查看系统中已经安装的软件" class="headerlink" title="查看系统中已经安装的软件"></a>查看系统中已经安装的软件</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1. yum list installed | grep &quot;软件名或者包名&quot;       # yum方法安装的</span><br><span class="line">2. rpm -qa | grep &quot;软件或者包的名字&quot;                # rpm包安装的</span><br><span class="line">3. dpkg -l | grep &quot;软件或者包的名字&quot;                # deb包安装的</span><br><span class="line">4. pip list                                        # pip安装的所有包</span><br><span class="line">5. 如果是以源码包自己编译安装的，例如.tar.gz或者tar.bz2形式的，只能看可执行文件是否存在了；</span><br><span class="line">   如果是以root用户安装的，可执行程序通常都在&#x2F;sbin:&#x2F;usr&#x2F;bin目录下</span><br><span class="line">6. 后面接 &#39;| wc -l&#39;                               #可以统计数量</span><br><span class="line">7. 后面接 &#39;| sort&#39;                                #排序</span><br></pre></td></tr></table></figure>


<hr>
<h6 id="清缓存"><a href="#清缓存" class="headerlink" title="清缓存"></a>清缓存</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sync</span><br><span class="line">echo 3 &gt; &#x2F;proc&#x2F;sys&#x2F;vm&#x2F;drop_caches</span><br><span class="line"></span><br><span class="line">0：不释放（系统默认值）</span><br><span class="line">1：释放页缓存</span><br><span class="line">2：释放dentries和inodes</span><br><span class="line">3：释放所有缓存</span><br></pre></td></tr></table></figure>


<hr>
<h5 id="拷贝命令"><a href="#拷贝命令" class="headerlink" title="拷贝命令"></a>拷贝命令</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">两台主机之间可以使用scp命令进行拷贝</span><br><span class="line">scp -r（递归） 需要拷贝的文件 用户@主机名或主机地址:&#x2F;opt</span><br><span class="line"></span><br><span class="line">在A主机上控制B主机拷贝文件到C主机上</span><br><span class="line">scp -r B主机用户名@主机名或主机地址:&#x2F;opt&#x2F;software C主机用户名@主机名或主机地址:&#x2F;opt&#x2F;software</span><br><span class="line"></span><br><span class="line">将A主机的文件拉取到本机上当前目录</span><br><span class="line">scp -r A主机用户名@主机名或主机地址:&#x2F;opt&#x2F;software .&#x2F;</span><br></pre></td></tr></table></figure>


<hr>
<h5 id="进程相关"><a href="#进程相关" class="headerlink" title="进程相关"></a>进程相关</h5><ul>
<li>netstat -anp|grep 进程号</li>
<li>查询xxxx的进程<ul>
<li>ps -ef | grep Name </li>
<li>ps -aux | grep Name</li>
</ul>
</li>
<li>通过pid查询端口<ul>
<li>netstat -nap | grep pid</li>
</ul>
</li>
<li>查监听的端口:<ul>
<li>netstat -ntpl |grep java</li>
</ul>
</li>
</ul>
<hr>
<h5 id="执行命令"><a href="#执行命令" class="headerlink" title="执行命令"></a>执行命令</h5><h6 id="执行gp脚本"><a href="#执行gp脚本" class="headerlink" title="执行gp脚本"></a>执行gp脚本</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">gpload -f gploader.yml</span><br><span class="line">sh wdxt_ncop1_gploader_test.sh</span><br></pre></td></tr></table></figure>


<hr>
<h5 id="解压命令"><a href="#解压命令" class="headerlink" title="解压命令"></a>解压命令</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1、*.tar                    #用 tar -xvf 解压</span><br><span class="line">2、*.gz                     #用 gzip -d 或 gunzip 解压</span><br><span class="line">3、*.tar.gz和*.tgz          #用 tar -xzf 解压</span><br><span class="line">4、*.bz2                    #用 bzip2 -d 或 用bunzip2 解压</span><br><span class="line">5、*.tar.bz2                #用 tar -xjf 解压</span><br><span class="line">6、*.Z                      #用 uncompress 解压</span><br><span class="line">7、*.tar.Z                  #用 tar -xZf 解压</span><br><span class="line">8、*.rar                    #用 unrar e 解压</span><br><span class="line">9、*.zip                    #用 unzip 解压</span><br><span class="line"></span><br><span class="line">unzip ngbomc-web-1.0-SNAPSHOT-r16569.war -d .&#x2F;ngbomc-web-1.0</span><br></pre></td></tr></table></figure>


<hr>
<h5 id="压缩命令（注：tar是打包，不是压缩！）"><a href="#压缩命令（注：tar是打包，不是压缩！）" class="headerlink" title="压缩命令（注：tar是打包，不是压缩！）"></a>压缩命令（注：tar是打包，不是压缩！）</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">tar cvf FileName.tar DirName</span><br><span class="line">tar zcvf ngbomc-cloud-itsm.tar.gz ngbomc-cloud-itsm</span><br></pre></td></tr></table></figure>



<hr>
<h5 id="CentOS切换图形化界面"><a href="#CentOS切换图形化界面" class="headerlink" title="CentOS切换图形化界面"></a>CentOS切换图形化界面</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">startx</span><br><span class="line">dos界面 ctrl+alt+F2切换回图形界面</span><br></pre></td></tr></table></figure>



<hr>
<h5 id="关机-amp-重启"><a href="#关机-amp-重启" class="headerlink" title="关机&amp;重启"></a>关机&amp;重启</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">重启		reboot</span><br><span class="line">关闭		halt 或者 shutdown</span><br></pre></td></tr></table></figure>



<hr>
<h4 id="一些情况下的通用参数"><a href="#一些情况下的通用参数" class="headerlink" title="一些情况下的通用参数"></a>一些情况下的通用参数</h4><ul>
<li>-R：递归</li>
<li>-P: 多级目录</li>
<li>-F: 强制？？？</li>
<li></li>
</ul>
<hr>
<h5 id="获取PID"><a href="#获取PID" class="headerlink" title="获取PID"></a>获取PID</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">方式一：通过端口获取-</span><br><span class="line"></span><br><span class="line">缺点:对于通过tomcat容器方式启动的话，可能会出现进程存在，但服务没有启动的情况，此时通过该方案无法通过端口无法获取到PID，导致进程存在的情况下再此启动；</span><br><span class="line">比如：</span><br><span class="line">ss -lntp sport &#x3D; :8080 | awk &#39;NR &gt; 1 &#123;print $NF&#125;&#39; | cut -d &#39;,&#39; -f 2 | head -n 1 | sed &#39;s&#x2F;pid&#x3D;&#x2F;&#x2F;&#39;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">方式二：通过路径获取</span><br><span class="line">一定要保证路径的唯一才可以，不然会出现获取多个pid的情况，需要根据具体情况具体处理。</span><br><span class="line">比如：</span><br><span class="line">ps -ef|grep &#x2F;server&#x2F;tomcat&#x2F;bin |grep -v grep|awk &#39;&#123;print $2&#125;&#39;</span><br><span class="line"></span><br><span class="line">ps -ef | grep $serviceName | grep -v grep | awk &#39;&#123;print $2&#125;&#39;</span><br></pre></td></tr></table></figure>



]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>shell</tag>
      </tags>
  </entry>
  <entry>
    <title>【MySql-远程访问】</title>
    <url>/mysql-remote.html</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><a id="more"></a>

<h4 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h4><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">登录数据库：mysql -u root -p</span><br><span class="line"></span><br><span class="line">使用数据库：<span class="keyword">use</span> mysql; </span><br><span class="line"></span><br><span class="line">查看用户：<span class="keyword">select</span> host, <span class="keyword">user</span> <span class="keyword">from</span> <span class="keyword">user</span>; </span><br><span class="line"></span><br><span class="line">创建用户：<span class="keyword">CREATE</span> <span class="keyword">USER</span> <span class="string">'ambow'</span>@<span class="string">'%'</span> <span class="keyword">IDENTIFIED</span> <span class="keyword">BY</span> <span class="string">'ambow123'</span>;<span class="comment">#远程登录 </span></span><br><span class="line"> //<span class="keyword">CREATE</span> <span class="keyword">USER</span> <span class="string">'test'</span>@<span class="string">'localhost'</span>  <span class="keyword">IDENTIFIED</span> <span class="keyword">BY</span> <span class="string">'1234'</span>; <span class="comment">#本地登录  </span></span><br><span class="line"></span><br><span class="line">授权：<span class="keyword">GRANT</span> <span class="keyword">ALL</span> <span class="keyword">PRIVILEGES</span> <span class="keyword">ON</span> ambowbms.* <span class="keyword">TO</span> <span class="string">'ambow'</span>@<span class="string">'%'</span> <span class="keyword">IDENTIFIED</span> <span class="keyword">BY</span> <span class="string">'ambow123'</span> <span class="keyword">WITH</span> <span class="keyword">GRANT</span> <span class="keyword">OPTION</span>;</span><br><span class="line"> //<span class="keyword">GRANT</span> <span class="keyword">ALL</span> <span class="keyword">PRIVILEGES</span> <span class="keyword">ON</span> *.* <span class="keyword">TO</span> <span class="string">'ambow'</span>@<span class="string">'%'</span> <span class="keyword">IDENTIFIED</span> <span class="keyword">BY</span> <span class="string">'ambow123'</span> <span class="keyword">WITH</span> <span class="keyword">GRANT</span> <span class="keyword">OPTION</span>;</span><br><span class="line"></span><br><span class="line">查看权限：<span class="keyword">show</span> <span class="keyword">grants</span> <span class="keyword">for</span> <span class="string">'ambow'</span>@<span class="string">'%'</span>;</span><br><span class="line"></span><br><span class="line">撤销权限：<span class="keyword">revoke</span> <span class="keyword">ALL</span> <span class="keyword">PRIVILEGES</span> <span class="keyword">on</span> *.* <span class="keyword">from</span> <span class="string">'ambow'</span>@<span class="string">'%'</span>;</span><br><span class="line"> //<span class="keyword">revoke</span> <span class="keyword">privileges</span> <span class="keyword">on</span> *.* <span class="keyword">from</span> <span class="string">'ambow'</span>@<span class="string">'%'</span>;</span><br><span class="line"></span><br><span class="line">删除用户：<span class="keyword">delete</span> <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">where</span> <span class="keyword">user</span>=<span class="string">'ambow'</span> <span class="keyword">and</span> host=<span class="string">'%'</span> ;</span><br><span class="line"> //<span class="keyword">drop</span> <span class="keyword">user</span> <span class="string">"ambow"</span>@<span class="string">"%"</span>;</span><br><span class="line"></span><br><span class="line">刷新权限表：（操作后必须要执行）</span><br><span class="line">     <span class="keyword">FLUSH</span> <span class="keyword">PRIVILEGES</span>;</span><br><span class="line"></span><br><span class="line">退出：exit;</span><br></pre></td></tr></table></figure>

<h4 id="注："><a href="#注：" class="headerlink" title="注："></a>注：</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">all privileges：表示将所有权限授予给用户。也可指定具体的权限，如：SELECT、CREATE、DROP等。</span><br><span class="line">on：表示这些权限对哪些数据库和表生效，格式：数据库名.表名，这里写“*”表示所有数据库，所有表。如果我要指定将权限应用到test库的user表中，可以这么写：test.user</span><br><span class="line">to：将权限授予哪个用户。格式：”用户名”@”登录IP或域名”。%表示没有限制，在任何主机都可以登录。比如：”yangxin”@”192.168.0.%”，表示yangxin这个用户只能在192.168.0IP段登录</span><br><span class="line">identified by：指定用户的登录密码</span><br><span class="line">with grant option：表示允许用户将自己的权限授权给其它用户</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>mysql</category>
      </categories>
      <tags>
        <tag>database</tag>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>【REDIS配置详解】</title>
    <url>/redis-config.html</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="实现功能"><a href="#实现功能" class="headerlink" title="实现功能"></a>实现功能</h2><ul>
<li>详解</li>
</ul>
<hr>
<a id="more"></a>

<h2 id="redis-config"><a href="#redis-config" class="headerlink" title="redis.config"></a>redis.config</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> redis 配置文件示例</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 当你需要为某个配置项指定内存大小的时候，必须要带上单位，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 通常的格式就是 1k 5gb 4m 等酱紫：</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 1k  =&gt; 1000 bytes</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 1kb =&gt; 1024 bytes</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 1m  =&gt; 1000000 bytes</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 1mb =&gt; 1024*1024 bytes</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 1g  =&gt; 1000000000 bytes</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 1gb =&gt; 1024*1024*1024 bytes</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 单位是不区分大小写的，你写 1K 5GB 4M 也行</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################# INCLUDES ###################################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 假如说你有一个可用于所有的 redis server 的标准配置模板，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 但针对某些 server 又需要一些个性化的设置，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 你可以使用 include 来包含一些其他的配置文件，这对你来说是非常有用的。</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 但是要注意哦，include 是不能被 config rewrite 命令改写的</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 由于 redis 总是以最后的加工线作为一个配置指令值，所以你最好是把 include 放在这个文件的最前面，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 以避免在运行时覆盖配置的改变，相反，你就把它放在后面（外国人真啰嗦）。</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> include /path/to/local.conf</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> include /path/to/other.conf</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################### 常用 #####################################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认情况下 redis 不是作为守护进程运行的，如果你想让它在后台运行，你就把它改成 yes。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 当redis作为守护进程运行的时候，它会写一个 pid 到 /var/run/redis.pid 文件里面。</span></span><br><span class="line">daemonize no</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 当redis作为守护进程运行的时候，它会把 pid 默认写到 /var/run/redis.pid 文件里面，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 但是你可以在这里自己制定它的文件位置。</span></span><br><span class="line">pidfile /var/run/redis.pid</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 监听端口号，默认为 6379，如果你设为 0 ，redis 将不在 socket 上监听任何客户端连接。</span></span><br><span class="line">port 6379</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> TCP 监听的最大容纳数量</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在高并发的环境下，你需要把这个值调高以避免客户端连接缓慢的问题。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Linux 内核会一声不响的把这个值缩小成 /proc/sys/net/core/somaxconn 对应的值，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 所以你要修改这两个值才能达到你的预期。</span></span><br><span class="line">tcp-backlog 511</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认情况下，redis 在 server 上所有有效的网络接口上监听客户端连接。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 你如果只想让它在一个网络接口上监听，那你就绑定一个IP或者多个IP。</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 示例，多个IP用空格隔开:</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">bind</span> 192.168.1.100 10.0.0.1</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">bind</span> 127.0.0.1</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定 unix socket 的路径。</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> unixsocket /tmp/redis.sock</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> unixsocketperm 755</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定在一个 client 空闲多少秒之后关闭连接（0 就是不管它）</span></span><br><span class="line">timeout 0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> tcp 心跳包。</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果设置为非零，则在与客户端缺乏通讯的时候使用 SO_KEEPALIVE 发送 tcp acks 给客户端。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这个之所有有用，主要由两个原因：</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 1) 防止死的 peers</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2) Take the connection alive from the point of view of network</span></span><br><span class="line"><span class="meta">#</span><span class="bash">    equipment <span class="keyword">in</span> the middle.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> On Linux, the specified value (<span class="keyword">in</span> seconds) is the period used to send ACKs.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Note that to close the connection the double of the time is needed.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> On other kernels the period depends on the kernel configuration.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> A reasonable value <span class="keyword">for</span> this option is 60 seconds.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 推荐一个合理的值就是60秒</span></span><br><span class="line">tcp-keepalive 0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 定义日志级别。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 可以是下面的这些值：</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> debug (适用于开发或测试阶段)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> verbose (many rarely useful info, but not a mess like the debug level)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> notice (适用于生产环境)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> warning (仅仅一些重要的消息被记录)</span></span><br><span class="line">loglevel notice</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 指定日志文件的位置</span></span><br><span class="line">logfile ""</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 要想把日志记录到系统日志，就把它改成 yes，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 也可以可选择性的更新其他的syslog 参数以达到你的要求</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> syslog-enabled no</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置 syslog 的 identity。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> syslog-ident redis</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置 syslog 的 facility，必须是 USER 或者是 LOCAL0-LOCAL7 之间的值。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> syslog-facility local0</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置数据库的数目。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认数据库是 DB 0，你可以在每个连接上使用 select &lt;dbid&gt; 命令选择一个不同的数据库，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 但是 dbid 必须是一个介于 0 到 databasees - 1 之间的值</span></span><br><span class="line">databases 16</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################### 快照 ################################</span></span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 存 DB 到磁盘：</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   格式：save &lt;间隔时间（秒）&gt; &lt;写入次数&gt;</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   根据给定的时间间隔和写入次数将数据保存到磁盘</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   下面的例子的意思是：</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   900 秒后如果至少有 1 个 key 的值变化，则保存</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   300 秒后如果至少有 10 个 key 的值变化，则保存</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   60 秒后如果至少有 10000 个 key 的值变化，则保存</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   注意：你可以注释掉所有的 save 行来停用保存功能。</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   也可以直接一个空字符串来实现停用：</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   save <span class="string">""</span></span></span><br><span class="line"></span><br><span class="line">save 900 1</span><br><span class="line">save 300 10</span><br><span class="line">save 60 10000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认情况下，如果 redis 最后一次的后台保存失败，redis 将停止接受写操作，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这样以一种强硬的方式让用户知道数据不能正确的持久化到磁盘，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 否则就会没人注意到灾难的发生。</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果后台保存进程重新启动工作了，redis 也将自动的允许写操作。</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 然而你要是安装了靠谱的监控，你可能不希望 redis 这样做，那你就改成 no 好了。</span></span><br><span class="line">stop-writes-on-bgsave-error yes</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 是否在 dump .rdb 数据库的时候使用 LZF 压缩字符串</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认都设为 yes</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果你希望保存子进程节省点 cpu ，你就设置它为 no ，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 不过这个数据集可能就会比较大</span></span><br><span class="line">rdbcompression yes</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 是否校验rdb文件</span></span><br><span class="line">rdbchecksum yes</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置 dump 的文件位置</span></span><br><span class="line">dbfilename dump.rdb</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 工作目录</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 例如上面的 dbfilename 只指定了文件名，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 但是它会写入到这个目录下。这个配置项一定是个目录，而不能是文件名。</span></span><br><span class="line">dir ./</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################ 主从复制 #################################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 主从复制。使用 slaveof 来让一个 redis 实例成为另一个reids 实例的副本。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 注意这个只需要在 slave 上配置。</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> slaveof &lt;masterip&gt; &lt;masterport&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果 master 需要密码认证，就在这里设置</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> masterauth &lt;master-password&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 当一个 slave 与 master 失去联系，或者复制正在进行的时候，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> slave 可能会有两种表现：</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 1) 如果为 yes ，slave 仍然会应答客户端请求，但返回的数据可能是过时，</span></span><br><span class="line"><span class="meta">#</span><span class="bash">    或者数据可能是空的在第一次同步的时候</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2) 如果为 no ，在你执行除了 info he salveof 之外的其他命令时，</span></span><br><span class="line"><span class="meta">#</span><span class="bash">    slave 都将返回一个 <span class="string">"SYNC with master in progress"</span> 的错误，</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line">slave-serve-stale-data yes</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 你可以配置一个 slave 实体是否接受写入操作。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 通过写入操作来存储一些短暂的数据对于一个 slave 实例来说可能是有用的，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 因为相对从 master 重新同步数而言，据数据写入到 slave 会更容易被删除。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 但是如果客户端因为一个错误的配置写入，也可能会导致一些问题。</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 从 redis 2.6 版起，默认 slaves 都是只读的。</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Note: <span class="built_in">read</span> only slaves are not designed to be exposed to untrusted clients</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> on the internet. It<span class="string">'s just a protection layer against misuse of the instance.</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Still a <span class="built_in">read</span> only slave exports by default all the administrative commands</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> such as CONFIG, DEBUG, and so forth. To a limited extent you can improve</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> security of <span class="built_in">read</span> only slaves using <span class="string">'rename-command'</span> to shadow all the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> administrative / dangerous commands.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 注意：只读的 slaves 没有被设计成在 internet 上暴露给不受信任的客户端。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 它仅仅是一个针对误用实例的一个保护层。</span></span><br><span class="line">slave-read-only yes</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Slaves 在一个预定义的时间间隔内发送 ping 命令到 server 。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 你可以改变这个时间间隔。默认为 10 秒。</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> repl-ping-slave-period 10</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The following option sets the replication timeout <span class="keyword">for</span>:</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置主从复制过期时间</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 1) Bulk transfer I/O during SYNC, from the point of view of slave.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2) Master timeout from the point of view of slaves (data, pings).</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 3) Slave timeout from the point of view of masters (REPLCONF ACK pings).</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> It is important to make sure that this value is greater than the value</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> specified <span class="keyword">for</span> repl-ping-slave-period otherwise a timeout will be detected</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> every time there is low traffic between the master and the slave.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这个值一定要比 repl-ping-slave-period 大</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> repl-timeout 60</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Disable TCP_NODELAY on the slave socket after SYNC?</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> If you select <span class="string">"yes"</span> Redis will use a smaller number of TCP packets and</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> less bandwidth to send data to slaves. But this can add a delay <span class="keyword">for</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the data to appear on the slave side, up to 40 milliseconds with</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Linux kernels using a default configuration.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> If you select <span class="string">"no"</span> the delay <span class="keyword">for</span> data to appear on the slave side will</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> be reduced but more bandwidth will be used <span class="keyword">for</span> replication.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> By default we optimize <span class="keyword">for</span> low latency, but <span class="keyword">in</span> very high traffic conditions</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> or when the master and slaves are many hops away, turning this to <span class="string">"yes"</span> may</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> be a good idea.</span></span><br><span class="line">repl-disable-tcp-nodelay no</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置主从复制容量大小。这个 backlog 是一个用来在 slaves 被断开连接时</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 存放 slave 数据的 buffer，所以当一个 slave 想要重新连接，通常不希望全部重新同步，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 只是部分同步就够了，仅仅传递 slave 在断开连接时丢失的这部分数据。</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The biggest the replication backlog, the longer the time the slave can be</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> disconnected and later be able to perform a partial resynchronization.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这个值越大，salve 可以断开连接的时间就越长。</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The backlog is only allocated once there is at least a slave connected.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> repl-backlog-size 1mb</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> After a master has no longer connected slaves <span class="keyword">for</span> some time, the backlog</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> will be freed. The following option configures the amount of seconds that</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> need to elapse, starting from the time the last slave disconnected, <span class="keyword">for</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the backlog buffer to be freed.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 在某些时候，master 不再连接 slaves，backlog 将被释放。</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> A value of 0 means to never release the backlog.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果设置为 0 ，意味着绝不释放 backlog 。</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> repl-backlog-ttl 3600</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 当 master 不能正常工作的时候，Redis Sentinel 会从 slaves 中选出一个新的 master，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 这个值越小，就越会被优先选中，但是如果是 0 ， 那是意味着这个 slave 不可能被选中。</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 默认优先级为 100。</span></span><br><span class="line">slave-priority 100</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> It is possible <span class="keyword">for</span> a master to stop accepting writes <span class="keyword">if</span> there are less than</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> N slaves connected, having a lag less or equal than M seconds.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The N slaves need to be <span class="keyword">in</span> <span class="string">"online"</span> state.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The lag <span class="keyword">in</span> seconds, that must be &lt;= the specified value, is calculated from</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the last ping received from the slave, that is usually sent every second.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> This option does not GUARANTEES that N replicas will accept the write, but</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> will <span class="built_in">limit</span> the window of exposure <span class="keyword">for</span> lost writes <span class="keyword">in</span> <span class="keyword">case</span> not enough slaves</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> are available, to the specified number of seconds.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> For example to require at least 3 slaves with a lag &lt;= 10 seconds use:</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> min-slaves-to-write 3</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> min-slaves-max-lag 10</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Setting one or the other to 0 disables the feature.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> By default min-slaves-to-write is <span class="built_in">set</span> to 0 (feature disabled) and</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> min-slaves-max-lag is <span class="built_in">set</span> to 10.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################# 安全 ###################################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Require clients to issue AUTH &lt;PASSWORD&gt; before processing any other</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> commands.  This might be useful <span class="keyword">in</span> environments <span class="keyword">in</span> <span class="built_in">which</span> you <span class="keyword">do</span> not trust</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> others with access to the host running redis-server.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> This should stay commented out <span class="keyword">for</span> backward compatibility and because most</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> people <span class="keyword">do</span> not need auth (e.g. they run their own servers).</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Warning: since Redis is pretty fast an outside user can try up to</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 150k passwords per second against a good box. This means that you should</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> use a very strong password otherwise it will be very easy to <span class="built_in">break</span>.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 设置认证密码</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> requirepass foobared</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Command renaming.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> It is possible to change the name of dangerous commands <span class="keyword">in</span> a shared</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> environment. For instance the CONFIG <span class="built_in">command</span> may be renamed into something</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> hard to guess so that it will still be available <span class="keyword">for</span> internal-use tools</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> but not available <span class="keyword">for</span> general clients.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Example:</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> rename-command CONFIG b840fc02d524045429941cc15f59e41cb7be6c52</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> It is also possible to completely <span class="built_in">kill</span> a <span class="built_in">command</span> by renaming it into</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> an empty string:</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> rename-command CONFIG <span class="string">""</span></span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Please note that changing the name of commands that are logged into the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> AOF file or transmitted to slaves may cause problems.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################## 限制 ####################################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Set the max number of connected clients at the same time. By default</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> this <span class="built_in">limit</span> is <span class="built_in">set</span> to 10000 clients, however <span class="keyword">if</span> the Redis server is not</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> able to configure the process file <span class="built_in">limit</span> to allow <span class="keyword">for</span> the specified <span class="built_in">limit</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the max number of allowed clients is <span class="built_in">set</span> to the current file <span class="built_in">limit</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> minus 32 (as Redis reserves a few file descriptors <span class="keyword">for</span> internal uses).</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 一旦达到最大限制，redis 将关闭所有的新连接</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 并发送一个‘max number of clients reached’的错误。</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> maxclients 10000</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果你设置了这个值，当缓存的数据容量达到这个值， redis 将根据你选择的</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> eviction 策略来移除一些 keys。</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果 redis 不能根据策略移除 keys ，或者是策略被设置为 ‘noeviction’，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> redis 将开始响应错误给命令，如 <span class="built_in">set</span>，lpush 等等，</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 并继续响应只读的命令，如 get</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> This option is usually useful when using Redis as an LRU cache, or to <span class="built_in">set</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> a hard memory <span class="built_in">limit</span> <span class="keyword">for</span> an instance (using the <span class="string">'noeviction'</span> policy).</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> WARNING: If you have slaves attached to an instance with maxmemory on,</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the size of the output buffers needed to feed the slaves are subtracted</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> from the used memory count, so that network problems / resyncs will</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> not trigger a loop <span class="built_in">where</span> keys are evicted, and <span class="keyword">in</span> turn the output</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> buffer of slaves is full with DELs of keys evicted triggering the deletion</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> of more keys, and so forth until the database is completely emptied.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> In short... <span class="keyword">if</span> you have slaves attached it is suggested that you <span class="built_in">set</span> a lower</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">limit</span> <span class="keyword">for</span> maxmemory so that there is some free RAM on the system <span class="keyword">for</span> slave</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> output buffers (but this is not needed <span class="keyword">if</span> the policy is <span class="string">'noeviction'</span>).</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 最大使用内存</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> maxmemory &lt;bytes&gt;</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 最大内存策略，你有 5 个选择。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> volatile-lru -&gt; remove the key with an expire <span class="built_in">set</span> using an LRU algorithm</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> volatile-lru -&gt; 使用 LRU 算法移除包含过期设置的 key 。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> allkeys-lru -&gt; remove any key accordingly to the LRU algorithm</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> allkeys-lru -&gt; 根据 LRU 算法移除所有的 key 。</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> volatile-random -&gt; remove a random key with an expire <span class="built_in">set</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> allkeys-random -&gt; remove a random key, any key</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> volatile-ttl -&gt; remove the key with the nearest expire time (minor TTL)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> noeviction -&gt; don<span class="string">'t expire at all, just return an error on write operations</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> noeviction -&gt; 不让任何 key 过期，只是给写入操作返回一个错误</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Note: with any of the above policies, Redis will <span class="built_in">return</span> an error on write</span></span><br><span class="line"><span class="meta">#</span><span class="bash">       operations, when there are not suitable keys <span class="keyword">for</span> eviction.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash">       At the date of writing this commands are: <span class="built_in">set</span> setnx setex append</span></span><br><span class="line"><span class="meta">#</span><span class="bash">       incr decr rpush lpush rpushx lpushx linsert lset rpoplpush sadd</span></span><br><span class="line"><span class="meta">#</span><span class="bash">       sinter sinterstore sunion sunionstore sdiff sdiffstore zadd zincrby</span></span><br><span class="line"><span class="meta">#</span><span class="bash">       zunionstore zinterstore hset hsetnx hmset hincrby incrby decrby</span></span><br><span class="line"><span class="meta">#</span><span class="bash">       getset mset msetnx <span class="built_in">exec</span> sort</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The default is:</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> maxmemory-policy noeviction</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> LRU and minimal TTL algorithms are not precise algorithms but approximated</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> algorithms (<span class="keyword">in</span> order to save memory), so you can tune it <span class="keyword">for</span> speed or</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> accuracy. For default Redis will check five keys and pick the one that was</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> used less recently, you can change the sample size using the following</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> configuration directive.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The default of 5 produces good enough results. 10 Approximates very closely</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="literal">true</span> LRU but costs a bit more CPU. 3 is very fast but not very accurate.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> maxmemory-samples 5</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################# APPEND ONLY MODE ###############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> By default Redis asynchronously dumps the dataset on disk. This mode is</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> good enough <span class="keyword">in</span> many applications, but an issue with the Redis process or</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> a power outage may result into a few minutes of writes lost (depending on</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the configured save points).</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The Append Only File is an alternative persistence mode that provides</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> much better durability. For instance using the default data fsync policy</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> (see later <span class="keyword">in</span> the config file) Redis can lose just one second of writes <span class="keyword">in</span> a</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> dramatic event like a server power outage, or a single write <span class="keyword">if</span> something</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> wrong with the Redis process itself happens, but the operating system is</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> still running correctly.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> AOF and RDB persistence can be enabled at the same time without problems.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> If the AOF is enabled on startup Redis will load the AOF, that is the file</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> with the better durability guarantees.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Please check http://redis.io/topics/persistence <span class="keyword">for</span> more information.</span></span><br><span class="line"></span><br><span class="line">appendonly no</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The name of the append only file (default: <span class="string">"appendonly.aof"</span>)</span></span><br><span class="line"></span><br><span class="line">appendfilename "appendonly.aof"</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The fsync() call tells the Operating System to actually write data on disk</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> instead to <span class="built_in">wait</span> <span class="keyword">for</span> more data <span class="keyword">in</span> the output buffer. Some OS will really flush </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> data on disk, some other OS will just try to <span class="keyword">do</span> it ASAP.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Redis supports three different modes:</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> no: don<span class="string">'t fsync, just let the OS flush the data when it wants. Faster.</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> always: fsync after every write to the append only <span class="built_in">log</span> . Slow, Safest.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> everysec: fsync only one time every second. Compromise.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The default is <span class="string">"everysec"</span>, as that<span class="string">'s usually the right compromise between</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> speed and data safety. It<span class="string">'s up to you to understand if you can relax this to</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="string">"no"</span> that will <span class="built_in">let</span> the operating system flush the output buffer when</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> it wants, <span class="keyword">for</span> better performances (but <span class="keyword">if</span> you can live with the idea of</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> some data loss consider the default persistence mode that<span class="string">'s snapshotting),</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> or on the contrary, use <span class="string">"always"</span> that<span class="string">'s very slow but a bit safer than</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> everysec.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> More details please check the following article:</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> http://antirez.com/post/redis-persistence-demystified.html</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> If unsure, use <span class="string">"everysec"</span>.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> appendfsync always</span></span><br><span class="line">appendfsync everysec</span><br><span class="line"><span class="meta">#</span><span class="bash"> appendfsync no</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> When the AOF fsync policy is <span class="built_in">set</span> to always or everysec, and a background</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> saving process (a background save or AOF <span class="built_in">log</span> background rewriting) is</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> performing a lot of I/O against the disk, <span class="keyword">in</span> some Linux configurations</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Redis may block too long on the fsync() call. Note that there is no fix <span class="keyword">for</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> this currently, as even performing fsync <span class="keyword">in</span> a different thread will block</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> our synchronous write(2) call.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> In order to mitigate this problem it<span class="string">'s possible to use the following option</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> that will prevent fsync() from being called <span class="keyword">in</span> the main process <span class="keyword">while</span> a</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> BGSAVE or BGREWRITEAOF is <span class="keyword">in</span> progress.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> This means that <span class="keyword">while</span> another child is saving, the durability of Redis is</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the same as <span class="string">"appendfsync none"</span>. In practical terms, this means that it is</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> possible to lose up to 30 seconds of <span class="built_in">log</span> <span class="keyword">in</span> the worst scenario (with the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> default Linux settings).</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> If you have latency problems turn this to <span class="string">"yes"</span>. Otherwise leave it as</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="string">"no"</span> that is the safest pick from the point of view of durability.</span></span><br><span class="line"></span><br><span class="line">no-appendfsync-on-rewrite no</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Automatic rewrite of the append only file.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Redis is able to automatically rewrite the <span class="built_in">log</span> file implicitly calling</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> BGREWRITEAOF when the AOF <span class="built_in">log</span> size grows by the specified percentage.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> This is how it works: Redis remembers the size of the AOF file after the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> latest rewrite (<span class="keyword">if</span> no rewrite has happened since the restart, the size of</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the AOF at startup is used).</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> This base size is compared to the current size. If the current size is</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> bigger than the specified percentage, the rewrite is triggered. Also</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> you need to specify a minimal size <span class="keyword">for</span> the AOF file to be rewritten, this</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> is useful to avoid rewriting the AOF file even <span class="keyword">if</span> the percentage increase</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> is reached but it is still pretty small.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Specify a percentage of zero <span class="keyword">in</span> order to <span class="built_in">disable</span> the automatic AOF</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> rewrite feature.</span></span><br><span class="line"></span><br><span class="line">auto-aof-rewrite-percentage 100</span><br><span class="line">auto-aof-rewrite-min-size 64mb</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################### LUA SCRIPTING  ###############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Max execution time of a Lua script <span class="keyword">in</span> milliseconds.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> If the maximum execution time is reached Redis will <span class="built_in">log</span> that a script is</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> still <span class="keyword">in</span> execution after the maximum allowed time and will start to</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> reply to queries with an error.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> When a long running script exceed the maximum execution time only the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> SCRIPT KILL and SHUTDOWN NOSAVE commands are available. The first can be</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> used to stop a script that did not yet called write commands. The second</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> is the only way to shut down the server <span class="keyword">in</span> the <span class="keyword">case</span> a write commands was</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> already issue by the script but the user don<span class="string">'t want to wait for the natural</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> termination of the script.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Set it to 0 or a negative value <span class="keyword">for</span> unlimited execution without warnings.</span></span><br><span class="line">lua-time-limit 5000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################### REDIS 集群  ###############################</span></span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 启用或停用集群</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cluster-enabled yes</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Every cluster node has a cluster configuration file. This file is not</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> intended to be edited by hand. It is created and updated by Redis nodes.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Every Redis Cluster node requires a different cluster configuration file.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Make sure that instances running <span class="keyword">in</span> the same system does not have</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> overlapping cluster configuration file names.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cluster-config-file nodes-6379.conf</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Cluster node timeout is the amount of milliseconds a node must be unreachable </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">for</span> it to be considered <span class="keyword">in</span> failure state.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Most other internal time limits are multiple of the node timeout.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cluster-node-timeout 15000</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> A slave of a failing master will avoid to start a failover <span class="keyword">if</span> its data</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> looks too old.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> There is no simple way <span class="keyword">for</span> a slave to actually have a exact measure of</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> its <span class="string">"data age"</span>, so the following two checks are performed:</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 1) If there are multiple slaves able to failover, they exchange messages</span></span><br><span class="line"><span class="meta">#</span><span class="bash">    <span class="keyword">in</span> order to try to give an advantage to the slave with the best</span></span><br><span class="line"><span class="meta">#</span><span class="bash">    replication offset (more data from the master processed).</span></span><br><span class="line"><span class="meta">#</span><span class="bash">    Slaves will try to get their rank by offset, and apply to the start</span></span><br><span class="line"><span class="meta">#</span><span class="bash">    of the failover a delay proportional to their rank.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 2) Every single slave computes the time of the last interaction with</span></span><br><span class="line"><span class="meta">#</span><span class="bash">    its master. This can be the last ping or <span class="built_in">command</span> received (<span class="keyword">if</span> the master</span></span><br><span class="line"><span class="meta">#</span><span class="bash">    is still <span class="keyword">in</span> the <span class="string">"connected"</span> state), or the time that elapsed since the</span></span><br><span class="line"><span class="meta">#</span><span class="bash">    disconnection with the master (<span class="keyword">if</span> the replication link is currently down).</span></span><br><span class="line"><span class="meta">#</span><span class="bash">    If the last interaction is too old, the slave will not try to failover</span></span><br><span class="line"><span class="meta">#</span><span class="bash">    at all.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The point <span class="string">"2"</span> can be tuned by user. Specifically a slave will not perform</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the failover <span class="keyword">if</span>, since the last interaction with the master, the time</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> elapsed is greater than:</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash">   (node-timeout * slave-validity-factor) + repl-ping-slave-period</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> So <span class="keyword">for</span> example <span class="keyword">if</span> node-timeout is 30 seconds, and the slave-validity-factor</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> is 10, and assuming a default repl-ping-slave-period of 10 seconds, the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> slave will not try to failover <span class="keyword">if</span> it was not able to talk with the master</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">for</span> longer than 310 seconds.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> A large slave-validity-factor may allow slaves with too old data to failover</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> a master, <span class="keyword">while</span> a too small value may prevent the cluster from being able to</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> elect a slave at all.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> For maximum availability, it is possible to <span class="built_in">set</span> the slave-validity-factor</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> to a value of 0, <span class="built_in">which</span> means, that slaves will always try to failover the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> master regardless of the last time they interacted with the master.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> (However they<span class="string">'ll always try to apply a delay proportional to their</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> offset rank).</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Zero is the only value able to guarantee that when all the partitions heal</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the cluster will always be able to <span class="built_in">continue</span>.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cluster-slave-validity-factor 10</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Cluster slaves are able to migrate to orphaned masters, that are masters</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> that are left without working slaves. This improves the cluster ability</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> to resist to failures as otherwise an orphaned master can<span class="string">'t be failed over</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">in</span> <span class="keyword">case</span> of failure <span class="keyword">if</span> it has no working slaves.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Slaves migrate to orphaned masters only <span class="keyword">if</span> there are still at least a</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> given number of other working slaves <span class="keyword">for</span> their old master. This number</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> is the <span class="string">"migration barrier"</span>. A migration barrier of 1 means that a slave</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> will migrate only <span class="keyword">if</span> there is at least 1 other working slave <span class="keyword">for</span> its master</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> and so forth. It usually reflects the number of slaves you want <span class="keyword">for</span> every</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> master <span class="keyword">in</span> your cluster.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Default is 1 (slaves migrate only <span class="keyword">if</span> their masters remain with at least</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> one slave). To <span class="built_in">disable</span> migration just <span class="built_in">set</span> it to a very large value.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> A value of 0 can be <span class="built_in">set</span> but is useful only <span class="keyword">for</span> debugging and dangerous</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">in</span> production.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> cluster-migration-barrier 1</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> In order to setup your cluster make sure to <span class="built_in">read</span> the documentation</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> available at http://redis.io web site.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">################################# SLOW LOG ###################################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The Redis Slow Log is a system to <span class="built_in">log</span> queries that exceeded a specified</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> execution time. The execution time does not include the I/O operations</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> like talking with the client, sending the reply and so forth,</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> but just the time needed to actually execute the <span class="built_in">command</span> (this is the only</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> stage of <span class="built_in">command</span> execution <span class="built_in">where</span> the thread is blocked and can not serve</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> other requests <span class="keyword">in</span> the meantime).</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> You can configure the slow <span class="built_in">log</span> with two parameters: one tells Redis</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> what is the execution time, <span class="keyword">in</span> microseconds, to exceed <span class="keyword">in</span> order <span class="keyword">for</span> the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">command</span> to get logged, and the other parameter is the length of the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> slow <span class="built_in">log</span>. When a new <span class="built_in">command</span> is logged the oldest one is removed from the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> queue of logged commands.</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The following time is expressed <span class="keyword">in</span> microseconds, so 1000000 is equivalent</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> to one second. Note that a negative number disables the slow <span class="built_in">log</span>, <span class="keyword">while</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> a value of zero forces the logging of every <span class="built_in">command</span>.</span></span><br><span class="line">slowlog-log-slower-than 10000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> There is no <span class="built_in">limit</span> to this length. Just be aware that it will consume memory.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> You can reclaim memory used by the slow <span class="built_in">log</span> with SLOWLOG RESET.</span></span><br><span class="line">slowlog-max-len 128</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################ Event notification ##############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Redis can notify Pub/Sub clients about events happening <span class="keyword">in</span> the key space.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> This feature is documented at http://redis.io/topics/keyspace-events</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> For instance <span class="keyword">if</span> keyspace events notification is enabled, and a client</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> performs a DEL operation on key <span class="string">"foo"</span> stored <span class="keyword">in</span> the Database 0, two</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> messages will be published via Pub/Sub:</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> PUBLISH __keyspace@0__:foo del</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> PUBLISH __keyevent@0__:del foo</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> It is possible to select the events that Redis will notify among a <span class="built_in">set</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> of classes. Every class is identified by a single character:</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  K     Keyspace events, published with __keyspace@&lt;db&gt;__ prefix.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  E     Keyevent events, published with __keyevent@&lt;db&gt;__ prefix.</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  g     Generic commands (non-type specific) like DEL, EXPIRE, RENAME, ...</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  $     String commands</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  l     List commands</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  s     Set commands</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  h     Hash commands</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  z     Sorted <span class="built_in">set</span> commands</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  x     Expired events (events generated every time a key expires)</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  e     Evicted events (events generated when a key is evicted <span class="keyword">for</span> maxmemory)</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  A     Alias <span class="keyword">for</span> g<span class="variable">$lshzxe</span>, so that the <span class="string">"AKE"</span> string means all the events.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  The <span class="string">"notify-keyspace-events"</span> takes as argument a string that is composed</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  by zero or multiple characters. The empty string means that notifications</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  are disabled at all.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  Example: to <span class="built_in">enable</span> list and generic events, from the point of view of the</span></span><br><span class="line"><span class="meta">#</span><span class="bash">           event name, use:</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  notify-keyspace-events Elg</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  Example 2: to get the stream of the expired keys subscribing to channel</span></span><br><span class="line"><span class="meta">#</span><span class="bash">             name __keyevent@0__:expired use:</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  notify-keyspace-events Ex</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash">  By default all notifications are disabled because most users don<span class="string">'t need</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">  this feature and the feature has some overhead. Note that <span class="keyword">if</span> you don<span class="string">'t</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash">  specify at least one of K or E, no events will be delivered.</span></span><br><span class="line">notify-keyspace-events ""</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment">############################## ADVANCED CONFIG ###############################</span></span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Hashes are encoded using a memory efficient data structure when they have a</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> small number of entries, and the biggest entry does not exceed a given</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> threshold. These thresholds can be configured using the following directives.</span></span><br><span class="line">hash-max-ziplist-entries 512</span><br><span class="line">hash-max-ziplist-value 64</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Similarly to hashes, small lists are also encoded <span class="keyword">in</span> a special way <span class="keyword">in</span> order</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> to save a lot of space. The special representation is only used when</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> you are under the following limits:</span></span><br><span class="line">list-max-ziplist-entries 512</span><br><span class="line">list-max-ziplist-value 64</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Sets have a special encoding <span class="keyword">in</span> just one <span class="keyword">case</span>: when a <span class="built_in">set</span> is composed</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> of just strings that happens to be integers <span class="keyword">in</span> radix 10 <span class="keyword">in</span> the range</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> of 64 bit signed integers.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The following configuration setting sets the <span class="built_in">limit</span> <span class="keyword">in</span> the size of the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">set</span> <span class="keyword">in</span> order to use this special memory saving encoding.</span></span><br><span class="line">set-max-intset-entries 512</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Similarly to hashes and lists, sorted sets are also specially encoded <span class="keyword">in</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> order to save a lot of space. This encoding is only used when the length and</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> elements of a sorted <span class="built_in">set</span> are below the following limits:</span></span><br><span class="line">zset-max-ziplist-entries 128</span><br><span class="line">zset-max-ziplist-value 64</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> HyperLogLog sparse representation bytes <span class="built_in">limit</span>. The <span class="built_in">limit</span> includes the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 16 bytes header. When an HyperLogLog using the sparse representation crosses</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> this <span class="built_in">limit</span>, it is converted into the dense representation.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> A value greater than 16000 is totally useless, since at that point the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> dense representation is more memory efficient.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The suggested value is ~ 3000 <span class="keyword">in</span> order to have the benefits of</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the space efficient encoding without slowing down too much PFADD,</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="built_in">which</span> is O(N) with the sparse encoding. The value can be raised to</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> ~ 10000 when CPU is not a concern, but space is, and the data <span class="built_in">set</span> is</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> composed of many HyperLogLogs with cardinality <span class="keyword">in</span> the 0 - 15000 range.</span></span><br><span class="line">hll-sparse-max-bytes 3000</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Active rehashing uses 1 millisecond every 100 milliseconds of CPU time <span class="keyword">in</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> order to <span class="built_in">help</span> rehashing the main Redis <span class="built_in">hash</span> table (the one mapping top-level</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> keys to values). The <span class="built_in">hash</span> table implementation Redis uses (see dict.c)</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> performs a lazy rehashing: the more operation you run into a <span class="built_in">hash</span> table</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> that is rehashing, the more rehashing <span class="string">"steps"</span> are performed, so <span class="keyword">if</span> the</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> server is idle the rehashing is never complete and some more memory is used</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> by the <span class="built_in">hash</span> table.</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> </span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The default is to use this millisecond 10 <span class="built_in">times</span> every second <span class="keyword">in</span> order to</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> active rehashing the main dictionaries, freeing memory when possible.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> If unsure:</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> use <span class="string">"activerehashing no"</span> <span class="keyword">if</span> you have hard latency requirements and it is</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> not a good thing <span class="keyword">in</span> your environment that Redis can reply form time to time</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> to queries with 2 milliseconds delay.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> use <span class="string">"activerehashing yes"</span> <span class="keyword">if</span> you don<span class="string">'t have such hard requirements but</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> want to free memory asap when possible.</span></span><br><span class="line">activerehashing yes</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> The client output buffer limits can be used to force disconnection of clients</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> that are not reading data from the server fast enough <span class="keyword">for</span> some reason (a</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> common reason is that a Pub/Sub client can<span class="string">'t consume messages as fast as the</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> publisher can produce them).</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The <span class="built_in">limit</span> can be <span class="built_in">set</span> differently <span class="keyword">for</span> the three different classes of clients:</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> normal -&gt; normal clients</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> slave  -&gt; slave clients and MONITOR clients</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> pubsub -&gt; clients subscribed to at least one pubsub channel or pattern</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The syntax of every client-output-buffer-limit directive is the following:</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> client-output-buffer-limit &lt;class&gt; &lt;hard <span class="built_in">limit</span>&gt; &lt;soft <span class="built_in">limit</span>&gt; &lt;soft seconds&gt;</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> A client is immediately disconnected once the hard <span class="built_in">limit</span> is reached, or <span class="keyword">if</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the soft <span class="built_in">limit</span> is reached and remains reached <span class="keyword">for</span> the specified number of</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> seconds (continuously).</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> So <span class="keyword">for</span> instance <span class="keyword">if</span> the hard <span class="built_in">limit</span> is 32 megabytes and the soft <span class="built_in">limit</span> is</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 16 megabytes / 10 seconds, the client will get disconnected immediately</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">if</span> the size of the output buffers reach 32 megabytes, but will also get</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> disconnected <span class="keyword">if</span> the client reaches 16 megabytes and continuously overcomes</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the <span class="built_in">limit</span> <span class="keyword">for</span> 10 seconds.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> By default normal clients are not limited because they don<span class="string">'t receive data</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> without asking (<span class="keyword">in</span> a push way), but just after a request, so only</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> asynchronous clients may create a scenario <span class="built_in">where</span> data is requested faster</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> than it can <span class="built_in">read</span>.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Instead there is a default <span class="built_in">limit</span> <span class="keyword">for</span> pubsub and slave clients, since</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> subscribers and slaves receive data <span class="keyword">in</span> a push fashion.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Both the hard or the soft <span class="built_in">limit</span> can be disabled by setting them to zero.</span></span><br><span class="line">client-output-buffer-limit normal 0 0 0</span><br><span class="line">client-output-buffer-limit slave 256mb 64mb 60</span><br><span class="line">client-output-buffer-limit pubsub 32mb 8mb 60</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> Redis calls an internal <span class="keyword">function</span> to perform many background tasks, like</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> closing connections of clients <span class="keyword">in</span> timeout, purging expired keys that are</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> never requested, and so forth.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Not all tasks are performed with the same frequency, but Redis checks <span class="keyword">for</span></span></span><br><span class="line"><span class="meta">#</span><span class="bash"> tasks to perform accordingly to the specified <span class="string">"hz"</span> value.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> By default <span class="string">"hz"</span> is <span class="built_in">set</span> to 10. Raising the value will use more CPU when</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> Redis is idle, but at the same time will make Redis more responsive when</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> there are many keys expiring at the same time, and timeouts may be</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> handled with more precision.</span></span><br><span class="line"><span class="meta">#</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> The range is between 1 and 500, however a value over 100 is usually not</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> a good idea. Most users should use the default of 10 and raise this up to</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> 100 only <span class="keyword">in</span> environments <span class="built_in">where</span> very low latency is required.</span></span><br><span class="line">hz 10</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> When a child rewrites the AOF file, <span class="keyword">if</span> the following option is enabled</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> the file will be fsync-ed every 32 MB of data generated. This is useful</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> <span class="keyword">in</span> order to commit the file to the disk more incrementally and avoid</span></span><br><span class="line"><span class="meta">#</span><span class="bash"> big latency spikes.</span></span><br><span class="line">aof-rewrite-incremental-fsync yes</span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>redis</category>
      </categories>
      <tags>
        <tag>redis</tag>
        <tag>config</tag>
      </tags>
  </entry>
  <entry>
    <title>【Solr入门--基于Tomcat搭建solr服务】</title>
    <url>/solr-01.html</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h3 id="1-准备环境"><a href="#1-准备环境" class="headerlink" title="1.准备环境"></a>1.准备环境</h3><h4 id="1-运行环境"><a href="#1-运行环境" class="headerlink" title="1.运行环境"></a>1.运行环境</h4><ul>
<li>主机：Linux</li>
<li>容器：tomcat<h4 id="2-安装solr服务"><a href="#2-安装solr服务" class="headerlink" title="2.安装solr服务"></a>2.安装solr服务</h4><h5 id="1-在主机下创建一个文件夹solr；"><a href="#1-在主机下创建一个文件夹solr；" class="headerlink" title="1.在主机下创建一个文件夹solr；"></a>1.在主机下创建一个文件夹solr；</h5><h5 id="2-在solr文件夹中安装tomcat；"><a href="#2-在solr文件夹中安装tomcat；" class="headerlink" title="2.在solr文件夹中安装tomcat；"></a>2.在solr文件夹中安装tomcat；</h5><h5 id="3-将solr-4-10-3-dist-solr-4-10-3-war复制到xxx-splr-tomcat-webapps-下，并改名为solr-war；"><a href="#3-将solr-4-10-3-dist-solr-4-10-3-war复制到xxx-splr-tomcat-webapps-下，并改名为solr-war；" class="headerlink" title="3.将solr-4.10.3/dist/solr-4.10.3.war复制到xxx/splr/tomcat/webapps/下，并改名为solr.war；"></a>3.将solr-4.10.3/dist/solr-4.10.3.war复制到xxx/splr/tomcat/webapps/下，并改名为solr.war；</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp solr-4.10.3.war xxx&#x2F;splr&#x2F;tomcat&#x2F;webapps&#x2F;solr.war</span><br></pre></td></tr></table></figure>

</li>
</ul>
<a id="more"></a>

<h5 id="4-启动tomcat以解压缩"><a href="#4-启动tomcat以解压缩" class="headerlink" title="4.启动tomcat以解压缩"></a>4.启动tomcat以解压缩</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">启动：</span><br><span class="line">bin&#x2F;startup.sh</span><br><span class="line">查看日志：</span><br><span class="line">tail -f logs&#x2F;catalina.out</span><br></pre></td></tr></table></figure>
<h5 id="5-关闭tomcat-删除没用的war包-删除时要保证tomcat已关闭"><a href="#5-关闭tomcat-删除没用的war包-删除时要保证tomcat已关闭" class="headerlink" title="5.关闭tomcat,删除没用的war包(删除时要保证tomcat已关闭)"></a>5.关闭tomcat,删除没用的war包(删除时要保证tomcat已关闭)</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;shutdown.sh</span><br><span class="line">rm -f solr.war</span><br></pre></td></tr></table></figure>
<h5 id="6-将example-lib-ext下的jar包拷贝到xxx-solr-tomcat-webapps-solr-WEB-INF-lib下-共5个日志包，否侧启动时会报错"><a href="#6-将example-lib-ext下的jar包拷贝到xxx-solr-tomcat-webapps-solr-WEB-INF-lib下-共5个日志包，否侧启动时会报错" class="headerlink" title="6.将example/lib/ext下的jar包拷贝到xxx/solr/tomcat/webapps/solr/WEB-INF/lib下(共5个日志包，否侧启动时会报错)"></a>6.将example/lib/ext下的jar包拷贝到xxx/solr/tomcat/webapps/solr/WEB-INF/lib下(共5个日志包，否侧启动时会报错)</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">jcl-over-slf4j-1.7.6.jar</span><br><span class="line">jul-to-slf4j-1.7.6.jar</span><br><span class="line">log4j-1.2.17.jar</span><br><span class="line">slf4j-api-1.7.6.jar</span><br><span class="line">slf4j-log4j12-1.7.6.jar</span><br></pre></td></tr></table></figure>
<h5 id="7-将example下的solr文件夹拷入solr文件夹下，更名为solrhome"><a href="#7-将example下的solr文件夹拷入solr文件夹下，更名为solrhome" class="headerlink" title="7.将example下的solr文件夹拷入solr文件夹下，更名为solrhome"></a>7.将example下的solr文件夹拷入solr文件夹下，更名为solrhome</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp -r solr xxx&#x2F;solr&#x2F;solrhome</span><br></pre></td></tr></table></figure>
<h5 id="8-更改配置，让solr知道solrhome在哪"><a href="#8-更改配置，让solr知道solrhome在哪" class="headerlink" title="8.更改配置，让solr知道solrhome在哪"></a>8.更改配置，让solr知道solrhome在哪</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">vim tomcat.webapps.solr&#x2F;WEB-INF&#x2F;web.xml</span><br><span class="line">解开注释，修改env-entry，将env-entry修改为solrhome真实所在位置</span><br></pre></td></tr></table></figure>
<h5 id="9-启动tomcat，访问solr后台：主机ip-8080-solr"><a href="#9-启动tomcat，访问solr后台：主机ip-8080-solr" class="headerlink" title="9.启动tomcat，访问solr后台：主机ip:8080/solr"></a>9.启动tomcat，访问solr后台：主机ip:8080/solr</h5><hr>
<h3 id="2-配置业务域"><a href="#2-配置业务域" class="headerlink" title="2.配置业务域"></a>2.配置业务域</h3><h4 id="1-为了搜索数据，创建域"><a href="#1-为了搜索数据，创建域" class="headerlink" title="1. 为了搜索数据，创建域"></a>1. 为了搜索数据，创建域</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">域：先定义，后使用；</span><br></pre></td></tr></table></figure>
<h4 id="2-根据数据，使用中文分析器分词"><a href="#2-根据数据，使用中文分析器分词" class="headerlink" title="2.根据数据，使用中文分析器分词"></a>2.根据数据，使用中文分析器分词</h4><h5 id="1-创建对应的业务域，需要定制中文分析器"><a href="#1-创建对应的业务域，需要定制中文分析器" class="headerlink" title="1.创建对应的业务域，需要定制中文分析器"></a>1.创建对应的业务域，需要定制中文分析器</h5><h6 id="1-将中文分析器添加到工程中。"><a href="#1-将中文分析器添加到工程中。" class="headerlink" title="1.将中文分析器添加到工程中。"></a>1.将中文分析器添加到工程中。</h6><ul>
<li>把IKAnalyzer2012FF_ul.jar添加到solr工程的lib目录下；</li>
<li>把扩展词典、配置文件放到solr工程的WEB_INF/classes目录下；<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">在WEB-INF下创建classes文件夹，并从拷贝：</span><br><span class="line">ext_stopword.dic</span><br><span class="line">IKAnalyzer.cfg.xml </span><br><span class="line">mydict.dic</span><br></pre></td></tr></table></figure>
<h6 id="2-因为要使用分析器进行分词，配置FileType，定制使用IKAnalyzer"><a href="#2-因为要使用分析器进行分词，配置FileType，定制使用IKAnalyzer" class="headerlink" title="2.因为要使用分析器进行分词，配置FileType，定制使用IKAnalyzer"></a>2.因为要使用分析器进行分词，配置FileType，定制使用IKAnalyzer</h6></li>
<li>在solrhome/collection1/conf/schema.xml配置域的类型和域的定义；</li>
<li>修改schema.xml文件，添加FiledType<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;!-- name可以自定义，class必须是TextField，只有TextField类型才能自定义分析器 --&gt;</span><br><span class="line">&lt;fieldType name&#x3D;&quot;text_ik&quot; class&#x3D;&quot;solr.TextField&quot;&gt;</span><br><span class="line">    &lt;analyzer class&#x3D;&quot;org.wltea.analyzer.luncene.IKAnalyzer&quot; &#x2F;&gt;</span><br><span class="line">&lt;&#x2F;fieldType&gt;</span><br></pre></td></tr></table></figure>
<h5 id="3-在schema-xml中-配置业务域，type制定使用自定义的FieldType"><a href="#3-在schema-xml中-配置业务域，type制定使用自定义的FieldType" class="headerlink" title="3.在schema.xml中,配置业务域，type制定使用自定义的FieldType"></a>3.在schema.xml中,配置业务域，type制定使用自定义的FieldType</h5></li>
<li>ID域不用定义，因为solr的每一个文档必须有一个ID,只需要自己的主键放到这个ID域即可</li>
<li>根据业务，将自己的其他业务域配置进来<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;!-- 业务域 --&gt;</span><br><span class="line">&lt;!-- 为要分词的字段设置type为fieldType的name --&gt;</span><br><span class="line">&lt;field name&#x3D;&quot;字段名称1&quot; type&#x3D;&quot;fieldType的name&quot; indexed&#x3D;&quot;true&quot; stored&#x3D;&quot;true&quot; &#x2F;&gt;</span><br><span class="line">&lt;!-- 不需要分词的type则按数据中设置  --&gt;</span><br><span class="line">&lt;field name&#x3D;&quot;price&quot; type&#x3D;&quot;long&quot; indexed&#x3D;&quot;true&quot; stored&#x3D;&quot;true&quot; &#x2F;&gt;</span><br><span class="line">&lt;field name&#x3D;&quot;image&quot; type&#x3D;&quot;string&quot; indexed&#x3D;&quot;true&quot; stored&#x3D;&quot;true&quot; &#x2F;&gt;</span><br></pre></td></tr></table></figure>
<h5 id="4-优化搜索，使用复制域"><a href="#4-优化搜索，使用复制域" class="headerlink" title="4.优化搜索，使用复制域"></a>4.优化搜索，使用复制域</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;!-- 复制域 --&gt;</span><br><span class="line">&lt;field name&#x3D;&quot;字段名称_keywords&quot; type&#x3D;&quot;fieldType的name&quot; indexed&#x3D;&quot;true&quot; stored&#x3D;&quot;true&quot; multivalued&#x3D;&quot;true&quot; &#x2F;&gt;</span><br><span class="line">&lt;copyField source&#x3D;&quot;字段名称1&quot; dest&#x3D;&quot;复制域name&quot; &#x2F;&gt;</span><br><span class="line">&lt;copyField source&#x3D;&quot;字段名称2&quot; dest&#x3D;&quot;复制域name&quot; &#x2F;&gt;</span><br></pre></td></tr></table></figure>
<h5 id="5-重启Tomcat"><a href="#5-重启Tomcat" class="headerlink" title="5.重启Tomcat"></a>5.重启Tomcat</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;shutdown.sh</span><br><span class="line">bin&#x2F;startup.sh</span><br></pre></td></tr></table></figure>
<h5 id="6-访问solr后台管理界面，点击Analysis查看分词器分词效果；"><a href="#6-访问solr后台管理界面，点击Analysis查看分词器分词效果；" class="headerlink" title="6.访问solr后台管理界面，点击Analysis查看分词器分词效果；"></a>6.访问solr后台管理界面，点击Analysis查看分词器分词效果；</h5><h5 id="7-在扩展词典中配置自己想要的特定词作为关键字；"><a href="#7-在扩展词典中配置自己想要的特定词作为关键字；" class="headerlink" title="7.在扩展词典中配置自己想要的特定词作为关键字；"></a>7.在扩展词典中配置自己想要的特定词作为关键字；</h5></li>
</ul>
<hr>
<h3 id="3-数据导入"><a href="#3-数据导入" class="headerlink" title="3.数据导入"></a>3.数据导入</h3><h4 id="1-Dataimport插件，在solr后台操作（本次不用）"><a href="#1-Dataimport插件，在solr后台操作（本次不用）" class="headerlink" title="1.Dataimport插件，在solr后台操作（本次不用）"></a>1.Dataimport插件，在solr后台操作（本次不用）</h4><h4 id="2-搭建搜索服务工程"><a href="#2-搭建搜索服务工程" class="headerlink" title="2.搭建搜索服务工程"></a>2.搭建搜索服务工程</h4><h5 id="1-新建Maven工程"><a href="#1-新建Maven工程" class="headerlink" title="1.新建Maven工程"></a>1.新建Maven工程</h5><h5 id="2-修改pom文件，修改端口号"><a href="#2-修改pom文件，修改端口号" class="headerlink" title="2.修改pom文件，修改端口号"></a>2.修改pom文件，修改端口号</h5><h5 id="3-将数据导入索引库"><a href="#3-将数据导入索引库" class="headerlink" title="3.将数据导入索引库"></a>3.将数据导入索引库</h5><h6 id="1-编写关联查询的sql查询数据"><a href="#1-编写关联查询的sql查询数据" class="headerlink" title="1.编写关联查询的sql查询数据"></a>1.编写关联查询的sql查询数据</h6><h6 id="2-。。。"><a href="#2-。。。" class="headerlink" title="2.。。。"></a>2.。。。</h6><h3 id="4-查看日志"><a href="#4-查看日志" class="headerlink" title="4.查看日志"></a>4.查看日志</h3><ul>
<li>tail -f catalina.out</li>
</ul>
]]></content>
      <categories>
        <category>solr</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>solr</tag>
      </tags>
  </entry>
  <entry>
    <title>【Solr入门--定时导入数据】</title>
    <url>/solr-04.html</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h5 id="1-将-apache-solr-dataimportscheduler-jar-和solr自带的-apache-solr-dataimporthandler-jar-apache-solr-dataimporthandler-extras-jar-放到solr-war的lib目录下面"><a href="#1-将-apache-solr-dataimportscheduler-jar-和solr自带的-apache-solr-dataimporthandler-jar-apache-solr-dataimporthandler-extras-jar-放到solr-war的lib目录下面" class="headerlink" title="1. 将 apache-solr-dataimportscheduler-.jar 和solr自带的 apache-solr-dataimporthandler-.jar, apache-solr-dataimporthandler-extras-.jar 放到solr.war的lib目录下面"></a>1. 将 apache-solr-dataimportscheduler-.jar 和solr自带的 apache-solr-dataimporthandler-.jar, apache-solr-dataimporthandler-extras-.jar 放到solr.war的lib目录下面</h5><h5 id="2-修改solr-war中WEB-INF-web-xml-在servlet节点前面增加，注意后面不要有空格"><a href="#2-修改solr-war中WEB-INF-web-xml-在servlet节点前面增加，注意后面不要有空格" class="headerlink" title="2.修改solr.war中WEB-INF/web.xml, 在servlet节点前面增加，注意后面不要有空格:"></a>2.修改solr.war中WEB-INF/web.xml, 在servlet节点前面增加，注意后面不要有空格:</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;listener&gt;</span><br><span class="line">    &lt;listener-class&gt;org.apache.solr.handler.dataimport.scheduler.ApplicationListener&lt;&#x2F;listener-class&gt;</span><br><span class="line">&lt;&#x2F;listener&gt;</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<h5 id="3-将apache-solr-dataimportscheduler-jar-中-dataimport-properties-取出并根据实际情况修改-然后放到-solr-home-conf-没有就手动创建；注意不是solr-home-core-conf-目录下面"><a href="#3-将apache-solr-dataimportscheduler-jar-中-dataimport-properties-取出并根据实际情况修改-然后放到-solr-home-conf-没有就手动创建；注意不是solr-home-core-conf-目录下面" class="headerlink" title="3.将apache-solr-dataimportscheduler-.jar 中 dataimport.properties 取出并根据实际情况修改,然后放到 solr.home/conf (没有就手动创建；注意不是solr.home/core/conf) 目录下面"></a>3.将apache-solr-dataimportscheduler-.jar 中 dataimport.properties 取出并根据实际情况修改,然后放到 solr.home/conf (没有就手动创建；注意不是solr.home/core/conf) 目录下面</h5><h6 id="dataimport-properties："><a href="#dataimport-properties：" class="headerlink" title="dataimport.properties："></a>dataimport.properties：</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#################################################</span><br><span class="line">#                                               #</span><br><span class="line">#       dataimport scheduler properties         #</span><br><span class="line">#                                               #</span><br><span class="line">#################################################</span><br><span class="line"></span><br><span class="line">#  to sync or not to sync</span><br><span class="line">#  1 - active; anything else - inactive</span><br><span class="line">syncEnabled&#x3D;1</span><br><span class="line"></span><br><span class="line">#  which cores to schedule</span><br><span class="line">#  in a multi-core environment you can decide which cores you want syncronized</span><br><span class="line">#  leave empty or comment it out if using single-core deployment</span><br><span class="line">#  改为所使用的core是哪个，可以写多个</span><br><span class="line">syncCores&#x3D;new_core</span><br><span class="line"></span><br><span class="line">#  solr server name or IP address</span><br><span class="line">#  [defaults to localhost if empty]</span><br><span class="line">server&#x3D;58.87.90.204</span><br><span class="line"></span><br><span class="line">#  solr server port</span><br><span class="line">#  [defaults to 80 if empty]</span><br><span class="line">port&#x3D;8080</span><br><span class="line"></span><br><span class="line">#  application name&#x2F;context</span><br><span class="line">#  [defaults to current ServletContextListener&#39;s context (app) name]</span><br><span class="line">webapp&#x3D;solr</span><br><span class="line"></span><br><span class="line">#  URL params [mandatory]</span><br><span class="line">#  remainder of URL</span><br><span class="line">#  solr同步数据时请求的链接</span><br><span class="line">params&#x3D;&#x2F;dataimport?command&#x3D;delta-import&amp;clean&#x3D;false&amp;commit&#x3D;true&amp;indent&#x3D;true&amp;wt&#x3D;json</span><br><span class="line"></span><br><span class="line">#  schedule interval</span><br><span class="line">#  number of minutes between two runs</span><br><span class="line">#  [defaults to 30 if empty]</span><br><span class="line"></span><br><span class="line">#这里是设置定时任务的，单位是分钟，也就是多长时间你检测一次数据同步，根据项目需求修改</span><br><span class="line">#开始测试的时候为了方便看到效果，时间可以设置短一点</span><br><span class="line">interval&#x3D;1</span><br><span class="line"></span><br><span class="line">#  重做索引的时间间隔，单位分钟，默认7200，即5天; </span><br><span class="line">#  为空,为0,或者注释掉:表示永不重做索引</span><br><span class="line">reBuildIndexInterval&#x3D;7200</span><br><span class="line"></span><br><span class="line">#  重做索引的参数</span><br><span class="line">#  reBuildIndexParams&#x3D;&#x2F;dataimport?command&#x3D;full-import&amp;clean&#x3D;true&amp;commit&#x3D;true</span><br><span class="line">#  reBuildIndexParams&#x3D;&#x2F;select?qt&#x3D;&#x2F;dataimport&amp;command&#x3D;full-import&amp;clean&#x3D;true&amp;commit&#x3D;true&amp;indent&#x3D;true&amp;wt&#x3D;json</span><br><span class="line">reBuildIndexParams&#x3D;&#x2F;dataimport?command&#x3D;full-import&amp;clean&#x3D;true&amp;commit&#x3D;true&amp;indent&#x3D;true&amp;wt&#x3D;json</span><br><span class="line"></span><br><span class="line">#  重做索引时间间隔的计时开始时间，第一次真正执行的时间&#x3D;reBuildIndexBeginTime+reBuildIndexInterval*60*1000；</span><br><span class="line">#  两种格式：2016-012-11 14:10:00 或者  03:10:00，后一种会自动补全日期部分为服务启动时的日期</span><br><span class="line">reBuildIndexBeginTime&#x3D;12:16:00</span><br></pre></td></tr></table></figure>

<h4 id="4-修改data-config-xml"><a href="#4-修改data-config-xml" class="headerlink" title="4.修改data-config.xml"></a>4.修改data-config.xml</h4><h6 id="增量式和全量通用写法"><a href="#增量式和全量通用写法" class="headerlink" title="增量式和全量通用写法"></a>增量式和全量通用写法</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;dataConfig&gt;</span><br><span class="line">    &lt;dataSource name&#x3D;&quot;mysource&quot; </span><br><span class="line">                type&#x3D;&quot;JdbcDataSource&quot; </span><br><span class="line">                driver&#x3D;&quot;com.mysql.jdbc.Driver&quot; </span><br><span class="line">                url&#x3D;&quot;jdbc:mysql:&#x2F;&#x2F;127.0.0.1:3306&#x2F;solr_181120&quot; </span><br><span class="line">                user&#x3D;&quot;root&quot; </span><br><span class="line">                password&#x3D;&quot;CHY0805&quot; </span><br><span class="line">                batchSize&#x3D;&quot;-1&quot; &#x2F;&gt;</span><br><span class="line">　　&lt;document&gt;</span><br><span class="line"></span><br><span class="line">         &lt;entity name&#x3D;&quot;mycore_books&quot; </span><br><span class="line">            pk&#x3D;&quot;id&quot; </span><br><span class="line">            dataSource&#x3D;&quot;mysource&quot; </span><br><span class="line">            query&#x3D;&quot;select id ,title,content,author,price,create_time from books&quot; </span><br><span class="line">            deltaImportQuery&#x3D;&quot;select * from books where id&#x3D;&#39;$&#123;dih.delta.id&#125;&#39;&quot; </span><br><span class="line">            deltaQuery&#x3D;&quot;select id from books where create_time &gt; &#39;$&#123;dih.last_index_time&#125;&#39;&quot;              </span><br><span class="line">            &gt;</span><br><span class="line">                          </span><br><span class="line">        &lt;!-- 每一个field映射着数据库中列与文档中的域，column是数据库列，name是solr的域(必须是在managed-schema文件中配置过的域才行) --&gt;</span><br><span class="line">            &lt;field column&#x3D;&quot;id&quot; name&#x3D;&quot;id&quot; &#x2F;&gt;</span><br><span class="line">            &lt;field column&#x3D;&quot;title&quot; name&#x3D;&quot;title&quot; &#x2F;&gt;</span><br><span class="line">            &lt;field column&#x3D;&quot;content&quot; name&#x3D;&quot;content&quot; &#x2F;&gt;</span><br><span class="line">            &lt;field column&#x3D;&quot;author&quot; name&#x3D;&quot;author&quot; &#x2F;&gt;            </span><br><span class="line">            &lt;field column&#x3D;&quot;price&quot; name&#x3D;&quot;price&quot; &#x2F;&gt;</span><br><span class="line">        &lt;field column&#x3D;&quot;create_time&quot; name&#x3D;&quot;create_time&quot; &#x2F;&gt;</span><br><span class="line">        &lt;&#x2F;entity&gt;</span><br><span class="line">        </span><br><span class="line">    &lt;&#x2F;document&gt;</span><br><span class="line">&lt;&#x2F;dataConfig&gt;</span><br></pre></td></tr></table></figure>

<h4 id="5-在solr5中解决时区的问题"><a href="#5-在solr5中解决时区的问题" class="headerlink" title="5.在solr5中解决时区的问题"></a>5.在solr5中解决时区的问题</h4><h6 id="直接修改配置文件bin-solr-in-sh即可"><a href="#直接修改配置文件bin-solr-in-sh即可" class="headerlink" title="直接修改配置文件bin/solr.in.sh即可"></a>直接修改配置文件bin/solr.in.sh即可</h6><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#SOLR_TIMEZONE&#x3D;&quot;UTC&quot;改为SOLR_TIMEZONE&#x3D;&quot;UTC+8&quot;</span><br></pre></td></tr></table></figure>

<h4 id="6-注意reBuildIndexBeginTime的时间格式，以及理解真正含义，免得苦苦等待"><a href="#6-注意reBuildIndexBeginTime的时间格式，以及理解真正含义，免得苦苦等待" class="headerlink" title="6.注意reBuildIndexBeginTime的时间格式，以及理解真正含义，免得苦苦等待"></a>6.注意reBuildIndexBeginTime的时间格式，以及理解真正含义，免得苦苦等待</h4>]]></content>
      <categories>
        <category>solr</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>solr</tag>
      </tags>
  </entry>
  <entry>
    <title>【Solr入门--导入数据（基于内置Jetty）】</title>
    <url>/solr-03.html</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h4 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1.准备工作"></a>1.准备工作</h4><h5 id="搭建配置好solr，新建一个core"><a href="#搭建配置好solr，新建一个core" class="headerlink" title="搭建配置好solr，新建一个core"></a>搭建配置好solr，新建一个core</h5><h4 id="2-修改solrconfig-xml"><a href="#2-修改solrconfig-xml" class="headerlink" title="2.修改solrconfig.xml"></a>2.修改solrconfig.xml</h4><h5 id="1-在的上面添加如下代码"><a href="#1-在的上面添加如下代码" class="headerlink" title="1.在的上面添加如下代码"></a>1.在<requestHandler name="/select" class="solr.SearchHandler">的上面添加如下代码</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;requestHandler name&#x3D;&quot;&#x2F;dataimport&quot; class&#x3D;&quot;org.apache.solr.handler.dataimport.DataImportHandler&quot;&gt;</span><br><span class="line">　　     &lt;lst name&#x3D;&quot;defaults&quot;&gt;</span><br><span class="line">　　        &lt;str name&#x3D;&quot;config&quot;&gt;data-config.xml&lt;&#x2F;str&gt;</span><br><span class="line">　　     &lt;&#x2F;lst&gt;</span><br><span class="line">　　&lt;&#x2F;requestHandler&gt;</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<h4 id="3-拷贝-home-centos-solr-solr-5-5-5-dist目录下的solr-dataimporthandler-5-5-5-jar和solr-dataimporthandler-extras-5-5-5-jar到-home-centos-solr-solr-5-5-5-server-solr-webapp-webapp-WEB-INF-lib下"><a href="#3-拷贝-home-centos-solr-solr-5-5-5-dist目录下的solr-dataimporthandler-5-5-5-jar和solr-dataimporthandler-extras-5-5-5-jar到-home-centos-solr-solr-5-5-5-server-solr-webapp-webapp-WEB-INF-lib下" class="headerlink" title="3.拷贝/home/centos/solr/solr-5.5.5/dist目录下的solr-dataimporthandler-5.5.5.jar和solr-dataimporthandler-extras-5.5.5.jar到/home/centos/solr/solr-5.5.5/server/solr-webapp/webapp/WEB-INF/lib下"></a>3.拷贝/home/centos/solr/solr-5.5.5/dist目录下的solr-dataimporthandler-5.5.5.jar和solr-dataimporthandler-extras-5.5.5.jar到/home/centos/solr/solr-5.5.5/server/solr-webapp/webapp/WEB-INF/lib下</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp solr-dataimporthandler-*.jar &#x2F;home&#x2F;centos&#x2F;solr&#x2F;solr-5.5.5&#x2F;server&#x2F;solr-webapp&#x2F;webapp&#x2F;WEB-INF&#x2F;lib</span><br></pre></td></tr></table></figure>
<h5 id="注：如果不拷贝jar，则在solrconfig-xml配置前，引入："><a href="#注：如果不拷贝jar，则在solrconfig-xml配置前，引入：" class="headerlink" title="注：如果不拷贝jar，则在solrconfig.xml配置前，引入："></a>注：如果不拷贝jar，则在solrconfig.xml配置前，引入：</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;!--引入DataImportHandler类的jar--&gt;</span><br><span class="line">&lt;lib dir&#x3D;&quot;$&#123;solr.install.dir:..&#x2F;..&#x2F;..&#x2F;..&#125;&#x2F;dist&#x2F;&quot; regex&#x3D;&quot;solr-dataimporthandler-.*\.jar&quot; &#x2F;&gt;</span><br></pre></td></tr></table></figure>

<h4 id="4-拷贝对应数据库驱动包到-home-centos-solr-solr-5-5-5-server-solr-webapp-webapp-WEB-INF-lib"><a href="#4-拷贝对应数据库驱动包到-home-centos-solr-solr-5-5-5-server-solr-webapp-webapp-WEB-INF-lib" class="headerlink" title="4.拷贝对应数据库驱动包到/home/centos/solr/solr-5.5.5/server/solr-webapp/webapp/WEB-INF/lib"></a>4.拷贝对应数据库驱动包到/home/centos/solr/solr-5.5.5/server/solr-webapp/webapp/WEB-INF/lib</h4><h4 id="5-连接主机Mysql"><a href="#5-连接主机Mysql" class="headerlink" title="5.连接主机Mysql"></a>5.连接主机Mysql</h4><ul>
<li>1.关闭防火墙或打开防火墙3306端口</li>
<li>2.授权远程连接</li>
</ul>
<h4 id="6-创建数据库、创建表"><a href="#6-创建数据库、创建表" class="headerlink" title="6.创建数据库、创建表"></a>6.创建数据库、创建表</h4><h4 id="7-在managed-schema中，配置数据库中对应字段"><a href="#7-在managed-schema中，配置数据库中对应字段" class="headerlink" title="7.在managed-schema中，配置数据库中对应字段"></a>7.在managed-schema中，配置数据库中对应字段</h4><h4 id="注：不要配置id-因为solr已经有id了"><a href="#注：不要配置id-因为solr已经有id了" class="headerlink" title="注：不要配置id,因为solr已经有id了"></a>注：不要配置id,因为solr已经有id了</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;field name&#x3D;&quot;title&quot; type&#x3D;&quot;text_ik&quot; indexed&#x3D;&quot;true&quot; stored&#x3D;&quot;true&quot;&#x2F;&gt;</span><br><span class="line">&lt;field name&#x3D;&quot;author&quot; type&#x3D;&quot;string&quot; indexed&#x3D;&quot;true&quot; stored&#x3D;&quot;true&quot;&#x2F;&gt;</span><br><span class="line">&lt;field name&#x3D;&quot;content&quot; type&#x3D;&quot;text_ik&quot; indexed&#x3D;&quot;true&quot;  stored&#x3D;&quot;true&quot; &#x2F;&gt;</span><br><span class="line">&lt;field name&#x3D;&quot;price&quot; type&#x3D;&quot;string&quot; indexed&#x3D;&quot;true&quot; stored&#x3D;&quot;true&quot;&#x2F;&gt;</span><br><span class="line">&lt;field name&#x3D;&quot;create_time&quot; type&#x3D;&quot;date&quot; indexed&#x3D;&quot;true&quot; stored&#x3D;&quot;true&quot;&#x2F;&gt;</span><br></pre></td></tr></table></figure>

<h4 id="8-在solrconfig-xml的同级目录下创建data-config-xml文件，然后配置数据库相关属性"><a href="#8-在solrconfig-xml的同级目录下创建data-config-xml文件，然后配置数据库相关属性" class="headerlink" title="8.在solrconfig.xml的同级目录下创建data-config.xml文件，然后配置数据库相关属性"></a>8.在solrconfig.xml的同级目录下创建data-config.xml文件，然后配置数据库相关属性</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;?xml version&#x3D;&quot;1.0&quot; encoding&#x3D;&quot;UTF-8&quot;?&gt;</span><br><span class="line">&lt;dataConfig&gt;</span><br><span class="line">    &lt;dataSource name&#x3D;&quot;mysource&quot; type&#x3D;&quot;JdbcDataSource&quot; driver&#x3D;&quot;com.mysql.jdbc.Driver&quot; url&#x3D;&quot;jdbc:mysql:&#x2F;&#x2F;127.0.0.1:3306&#x2F;solr_181120&quot; user&#x3D;&quot;root&quot; password&#x3D;&quot;CHY0805&quot; batchSize&#x3D;&quot;-1&quot; &#x2F;&gt;</span><br><span class="line">　　&lt;document&gt;</span><br><span class="line">        &lt;entity name&#x3D;&quot;mycore_books&quot; dataSource&#x3D;&quot;mysource&quot; pk&#x3D;&quot;id&quot; query&#x3D;&quot;select id ,title,content,author,price,create_time from books&quot;&gt;</span><br><span class="line">             &lt;!--column的id是数据库的id,name的id是managed_schema里面的id，id是必须，并且唯一的--&gt;</span><br><span class="line">            &lt;field column&#x3D;&quot;id&quot; name&#x3D;&quot;id&quot; &#x2F;&gt;</span><br><span class="line">             &lt;!--column的vip是数据库的vip字段,name的vip是managed_schema里面的vip,下面配置同理--&gt;</span><br><span class="line">            &lt;field column&#x3D;&quot;title&quot; name&#x3D;&quot;title&quot; &#x2F;&gt;</span><br><span class="line">            &lt;field column&#x3D;&quot;content&quot; name&#x3D;&quot;content&quot; &#x2F;&gt;</span><br><span class="line">            &lt;field column&#x3D;&quot;author&quot; name&#x3D;&quot;author&quot; &#x2F;&gt;            </span><br><span class="line">            &lt;field column&#x3D;&quot;price&quot; name&#x3D;&quot;price&quot; &#x2F;&gt;</span><br><span class="line">            &lt;field column&#x3D;&quot;create_time&quot; name&#x3D;&quot;create_time&quot; &#x2F;&gt;</span><br><span class="line">        &lt;&#x2F;entity&gt;</span><br><span class="line">    &lt;&#x2F;document&gt;</span><br><span class="line">&lt;&#x2F;dataConfig&gt;</span><br></pre></td></tr></table></figure>

<h5 id="9-重启solr"><a href="#9-重启solr" class="headerlink" title="9.重启solr"></a>9.重启solr</h5><h5 id="10-BINGO"><a href="#10-BINGO" class="headerlink" title="10.BINGO"></a>10.BINGO</h5><p><img src="pic.owlhy.com/blog/20200602225812.png" alt=""></p>
]]></content>
      <categories>
        <category>solr</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>solr</tag>
      </tags>
  </entry>
  <entry>
    <title>【Solr入门--搭建Solr过程中遇到的问题汇总】</title>
    <url>/solr-06.html</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h5 id="1-使用腾讯等云主机，有安全组的，如果不放行出站规则，-dataimport-properties中的请求地址不可以使用公网地址"><a href="#1-使用腾讯等云主机，有安全组的，如果不放行出站规则，-dataimport-properties中的请求地址不可以使用公网地址" class="headerlink" title="1.使用腾讯等云主机，有安全组的，如果不放行出站规则，.dataimport.properties中的请求地址不可以使用公网地址"></a>1.使用腾讯等云主机，有安全组的，如果不放行出站规则，.dataimport.properties中的请求地址不可以使用公网地址</h5><a id="more"></a>

<h5 id="2-dataimport-properties在parmams中不不配置-amp-indent-true-amp-wt-json会报404"><a href="#2-dataimport-properties在parmams中不不配置-amp-indent-true-amp-wt-json会报404" class="headerlink" title="2.dataimport.properties在parmams中不不配置&amp;indent=true&amp;wt=json会报404"></a>2.dataimport.properties在parmams中不不配置&amp;indent=true&amp;wt=json会报404</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">params&#x3D;&#x2F;dataimport?command&#x3D;delta-import&amp;clean&#x3D;false&amp;commit&#x3D;true&amp;indent&#x3D;true&amp;wt&#x3D;json</span><br></pre></td></tr></table></figure>

<h5 id="3-solr5中-时区差8小时问题"><a href="#3-solr5中-时区差8小时问题" class="headerlink" title="3.solr5中,时区差8小时问题"></a>3.solr5中,时区差8小时问题</h5><h5 id="4-搭建过程中，不要将文件夹直接放到-home目录下，要放到-home路径下的用户目录下，，否则。。。"><a href="#4-搭建过程中，不要将文件夹直接放到-home目录下，要放到-home路径下的用户目录下，，否则。。。" class="headerlink" title="4.搭建过程中，不要将文件夹直接放到/home目录下，要放到/home路径下的用户目录下，，否则。。。"></a>4.搭建过程中，不要将文件夹直接放到/home目录下，要放到/home路径下的用户目录下，，否则。。。</h5>]]></content>
      <categories>
        <category>solr</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>solr</tag>
      </tags>
  </entry>
  <entry>
    <title>【SpringBoot中logback处理日志】</title>
    <url>/springboot-logback.html</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h2 id="实现功能"><a href="#实现功能" class="headerlink" title="实现功能"></a>实现功能</h2><ul>
<li>每60M分割日志</li>
<li>按天分割日志</li>
<li>日志推送到LogStach</li>
</ul>
<hr>
<a id="more"></a>

<h2 id="logback-boot-xml"><a href="#logback-boot-xml" class="headerlink" title="logback-boot.xml"></a>logback-boot.xml</h2><figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="meta">&lt;?xml version="1.0" encoding="UTF-8" ?&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">configuration</span> <span class="attr">debug</span>=<span class="string">"true"</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="tag">&lt;<span class="name">conversionRule</span> <span class="attr">conversionWord</span>=<span class="string">"clr"</span> <span class="attr">converterClass</span>=<span class="string">"org.springframework.boot.logging.logback.ColorConverter"</span> /&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--格式化输出：%d表示日期，%thread表示线程名，%-5level：级别从左显示5个字符宽度%msg：日志消息，%n是换行符--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"PATTERN_COLOR"</span> <span class="attr">value</span>=<span class="string">"%-12(%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125;) |%clr(-%-5level) %clr([%X&#123;localIP&#125;]) %clr([%thread]) %yellow(at %class.%method) \\(%file:%line\\) %green([%X&#123;requestId&#125;]) %clr(-|) %highlight(%msg%n)"</span> /&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--去除--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"PATTERN"</span> <span class="attr">value</span>=<span class="string">"%d&#123;yyyy-MM-dd HH:mm:ss.SSS&#125; |-%-5level [%X&#123;localIP&#125;] [%thread] at %class.%method \\(%file:%line\\) [%X&#123;requestId&#125;] -| %msg%n"</span> /&gt;</span></span><br><span class="line">    <span class="comment">&lt;!--定义日志文件的存储地址 勿在 LogBack 的配置中使用相对路径--&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">property</span> <span class="attr">name</span>=<span class="string">"LOG_HOME"</span> <span class="attr">value</span>=<span class="string">"./logs"</span> /&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 控制台 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"CONSOLE"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.ConsoleAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;PATTERN_COLOR&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- SYS_INFO --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"SYS_INFO"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">File</span>&gt;</span>$&#123;LOG_HOME&#125;/info.log<span class="tag">&lt;/<span class="name">File</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!--过滤器,只打INFO级别的日志--&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.LevelFilter"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>INFO<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMatch</span>&gt;</span>ACCEPT<span class="tag">&lt;/<span class="name">onMatch</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">onMismatch</span>&gt;</span>DENY<span class="tag">&lt;/<span class="name">onMismatch</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">FileNamePattern</span>&gt;</span>$&#123;LOG_HOME&#125;/info/%d&#123;yyyy-MM,aux&#125;/info-%d&#123;yyyy-MM-dd&#125;.%i.log<span class="tag">&lt;/<span class="name">FileNamePattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">MaxHistory</span>&gt;</span>90<span class="tag">&lt;/<span class="name">MaxHistory</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">MaxFileSize</span>&gt;</span>60MB<span class="tag">&lt;/<span class="name">MaxFileSize</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">totalSizeCap</span>&gt;</span>5GB<span class="tag">&lt;/<span class="name">totalSizeCap</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;PATTERN&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- SYS_ERROR --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"SYS_ERROR"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">File</span>&gt;</span>$&#123;LOG_HOME&#125;/error.log<span class="tag">&lt;/<span class="name">File</span>&gt;</span></span><br><span class="line">        <span class="comment">&lt;!-- 按临界值过滤日志：低于ERROR以下级别被抛弃 --&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">filter</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.filter.ThresholdFilter"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">level</span>&gt;</span>ERROR<span class="tag">&lt;/<span class="name">level</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">filter</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">FileNamePattern</span>&gt;</span>$&#123;LOG_HOME&#125;/error/%d&#123;yyyy-MM,aux&#125;/error-%d&#123;yyyy-MM-dd&#125;.%i.log<span class="tag">&lt;/<span class="name">FileNamePattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">MaxHistory</span>&gt;</span>90<span class="tag">&lt;/<span class="name">MaxHistory</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">MaxFileSize</span>&gt;</span>60MB<span class="tag">&lt;/<span class="name">MaxFileSize</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">totalSizeCap</span>&gt;</span>5GB<span class="tag">&lt;/<span class="name">totalSizeCap</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;PATTERN&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- SYS_ALL --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"SYS_ALL"</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.RollingFileAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">File</span>&gt;</span>$&#123;LOG_HOME&#125;/../owlhy.log<span class="tag">&lt;/<span class="name">File</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">rollingPolicy</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.core.rolling.SizeAndTimeBasedRollingPolicy"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">FileNamePattern</span>&gt;</span>$&#123;LOG_HOME&#125;/owlhy/%d&#123;yyyy-MM,aux&#125;/owlhy-%d&#123;yyyy-MM-dd&#125;.%i.log<span class="tag">&lt;/<span class="name">FileNamePattern</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">MaxHistory</span>&gt;</span>90<span class="tag">&lt;/<span class="name">MaxHistory</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">MaxFileSize</span>&gt;</span>60MB<span class="tag">&lt;/<span class="name">MaxFileSize</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">totalSizeCap</span>&gt;</span>5GB<span class="tag">&lt;/<span class="name">totalSizeCap</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">rollingPolicy</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"ch.qos.logback.classic.encoder.PatternLayoutEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">pattern</span>&gt;</span>$&#123;PATTERN_COLOR&#125;<span class="tag">&lt;/<span class="name">pattern</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- LOGSTASH --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">appender</span> <span class="attr">name</span>=<span class="string">"LOGSTASH"</span> <span class="attr">class</span>=<span class="string">"net.logstash.logback.appender.LogstashTcpSocketAppender"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">destination</span>&gt;</span>10.1.192.188:9100<span class="tag">&lt;/<span class="name">destination</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">includeCallerData</span>&gt;</span>true<span class="tag">&lt;/<span class="name">includeCallerData</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">encoder</span> <span class="attr">class</span>=<span class="string">"net.logstash.logback.encoder.LogstashEncoder"</span>&gt;</span></span><br><span class="line">            <span class="tag">&lt;<span class="name">includeCallerData</span>&gt;</span>true<span class="tag">&lt;/<span class="name">includeCallerData</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;/<span class="name">encoder</span>&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">appender</span>&gt;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">&lt;!-- 日志级别从低到高分为: TRACE &lt; DEBUG &lt; INFO &lt; WARN &lt; ERROR &lt; FATAL --&gt;</span></span><br><span class="line">    <span class="comment">&lt;!-- 日志输出级别 --&gt;</span></span><br><span class="line">    <span class="tag">&lt;<span class="name">root</span> <span class="attr">level</span>=<span class="string">"INFO"</span>&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"CONSOLE"</span> /&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"SYS_INFO"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"SYS_ERROR"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"SYS_ALL"</span>/&gt;</span></span><br><span class="line">        <span class="tag">&lt;<span class="name">appender-ref</span> <span class="attr">ref</span>=<span class="string">"LOGSTASH"</span>/&gt;</span></span><br><span class="line">    <span class="tag">&lt;/<span class="name">root</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">configuration</span>&gt;</span></span><br></pre></td></tr></table></figure>
]]></content>
      <categories>
        <category>java</category>
        <category>springboot</category>
      </categories>
      <tags>
        <tag>springboot</tag>
        <tag>log</tag>
      </tags>
  </entry>
  <entry>
    <title>【Hadoop学习笔记 -- 修改本地临时文件存储目录】</title>
    <url>/hadoop-04.html</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h4 id="1-修改本地临时文件存储目录"><a href="#1-修改本地临时文件存储目录" class="headerlink" title="1.修改本地临时文件存储目录"></a>1.修改本地临时文件存储目录</h4><h5 id="1-停止进程"><a href="#1-停止进程" class="headerlink" title="1.停止进程"></a>1.停止进程</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#退出集群</span><br><span class="line">#1.退出ResourceManager和NodeManager:</span><br><span class="line">    sibn&#x2F;yarn-daemon.sh stop resourcemanager</span><br><span class="line">    sibn&#x2F;yarn-daemon.sh stop nodemanager</span><br><span class="line">    </span><br><span class="line">#2.退出NameNode和DataNode:</span><br><span class="line">    sibn&#x2F;hadoop-daemon.sh stop namenode</span><br><span class="line">    sibn&#x2F;hadoop-daemon.sh stop datanode</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<h5 id="2-修改hadoop-tmp-dir"><a href="#2-修改hadoop-tmp-dir" class="headerlink" title="2.修改hadoop.tmp.dir"></a>2.修改hadoop.tmp.dir</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># 在hadoop目录下新建文件夹</span><br><span class="line"># mkdir -p data&#x2F;tmp</span><br><span class="line"></span><br><span class="line">    cd etc&#x2F;hadoop</span><br><span class="line">    vim core-site.xml</span><br><span class="line"></span><br><span class="line"># 追加配置：</span><br><span class="line">    &lt;!-- 指定Hadoop运行时产生文件的存储目录 --&gt;</span><br><span class="line">    &lt;property&gt;</span><br><span class="line">        &lt;name&gt;hadoop.tmp.dir&lt;&#x2F;name&gt;</span><br><span class="line">        &lt;value&gt;&#x2F;opt&#x2F;module&#x2F;hadoop-2.7.2&#x2F;data&#x2F;tmp&lt;&#x2F;value&gt;</span><br><span class="line">    &lt;&#x2F;property&gt;</span><br></pre></td></tr></table></figure>
<h5 id="3-格式化NameNode"><a href="#3-格式化NameNode" class="headerlink" title="3.格式化NameNode"></a>3.格式化NameNode</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;hdfs namenode -format</span><br></pre></td></tr></table></figure>
<h5 id="4-重新启动hadoop"><a href="#4-重新启动hadoop" class="headerlink" title="4.重新启动hadoop"></a>4.重新启动hadoop</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#清除之前的缓存文件</span><br><span class="line">cd &#x2F;tmp</span><br><span class="line">rm -rf hadoop-giraffey375*</span><br><span class="line"></span><br><span class="line">cd hadoop-2.7.2</span><br><span class="line">rm -rf logs</span><br><span class="line"></span><br><span class="line">#启动</span><br><span class="line">sibn&#x2F;hadoop-daemon.sh start namenode</span><br><span class="line">sibn&#x2F;hadoop-daemon.sh start datanode</span><br><span class="line"></span><br><span class="line">sibn&#x2F;yarn-daemon.sh start resourcemanager</span><br><span class="line">sibn&#x2F;yarn-daemon.sh start nodemanager</span><br></pre></td></tr></table></figure>
<h5 id="5-上传测试文件"><a href="#5-上传测试文件" class="headerlink" title="5.上传测试文件"></a>5.上传测试文件</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop fs -mkdir -p &#x2F;user&#x2F;giraffey375&#x2F;input</span><br><span class="line">hadoop fs -put wcinput&#x2F;wc.input &#x2F;user&#x2F;giraffey375&#x2F;input</span><br><span class="line">hadoop fs -lsr &#x2F;</span><br></pre></td></tr></table></figure>

<hr>
<h4 id="默认配置文件详解"><a href="#默认配置文件详解" class="headerlink" title="默认配置文件详解"></a>默认配置文件详解</h4><h5 id="1-默认配置文件位置："><a href="#1-默认配置文件位置：" class="headerlink" title="1.默认配置文件位置："></a>1.默认配置文件位置：</h5><p><img src="pic.owlhy.com/blog/20200602223516.png" alt="images"></p>
]]></content>
      <categories>
        <category>hadoop</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>【Solr入门--基于内置Jetty搭建solr服务】</title>
    <url>/solr-02.html</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><h3 id="1-准备工作"><a href="#1-准备工作" class="headerlink" title="1.准备工作"></a>1.准备工作</h3><hr>
<h5 id="1-下载solr-5-5-5-tgz"><a href="#1-下载solr-5-5-5-tgz" class="headerlink" title="1.下载solr-5.5.5.tgz"></a>1.下载solr-5.5.5.tgz</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">下载地址:http:&#x2F;&#x2F;archive.apache.org&#x2F;dist&#x2F;lucene&#x2F;solr&#x2F;</span><br></pre></td></tr></table></figure>
<h5 id="2-创建一个目录solr-将solr解压过去"><a href="#2-创建一个目录solr-将solr解压过去" class="headerlink" title="2.创建一个目录solr,将solr解压过去"></a>2.创建一个目录solr,将solr解压过去</h5><h5 id="3-开放防火墙指定端口"><a href="#3-开放防火墙指定端口" class="headerlink" title="3.开放防火墙指定端口"></a>3.开放防火墙指定端口</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">参考CentOS6和CentOS7区别之防火墙</span><br></pre></td></tr></table></figure>

<a id="more"></a>

<h5 id="4-运行solr"><a href="#4-运行solr" class="headerlink" title="4.运行solr"></a>4.运行solr</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd bin</span><br><span class="line">.&#x2F;solr start -p 8080</span><br></pre></td></tr></table></figure>
<h5 id="5-访问solr后台管理"><a href="#5-访问solr后台管理" class="headerlink" title="5.访问solr后台管理"></a>5.访问solr后台管理</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">主机:端口(默认：8983)</span><br><span class="line">或</span><br><span class="line">主机:端口&#x2F;solr&#x2F;admin.html</span><br></pre></td></tr></table></figure>
<h5 id="注："><a href="#注：" class="headerlink" title="注："></a>注：</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1.本次搭建使用的是腾讯云主机，我之前在搭建个人博客时，使用了‘宝塔面板’，设置了出站入站的安全策略，所以在主机上直接添加开放端口时，端口可能并没有真的开放，所以导致一直访问不了</span><br><span class="line">2.使用：http:&#x2F;&#x2F;tool.chinaz.com&#x2F;port&#x2F;,扫描端口</span><br></pre></td></tr></table></figure>
<hr>
<h3 id="创建core"><a href="#创建core" class="headerlink" title="创建core"></a>创建core</h3><h4 id="方式1-使用create创建core"><a href="#方式1-使用create创建core" class="headerlink" title="方式1.使用create创建core"></a>方式1.使用create创建core</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin&#x2F;solr create -c  coreName(起一个名字)</span><br></pre></td></tr></table></figure>
<p><img src="pic.owlhy.com/blog/20200602225428.png" alt=""><br><img src="pic.owlhy.com/blog/20200602225447.png" alt=""><br><img src="pic.owlhy.com/blog/20200602225502.png" alt=""></p>
<hr>
<h4 id="方式-创建new-core–2"><a href="#方式-创建new-core–2" class="headerlink" title="方式.创建new_core–2"></a>方式.创建new_core–2</h4><p><img src="pic.owlhy.com/blog/20200602225528.png" alt=""><br><img src="pic.owlhy.com/blog/20200602225544.png" alt=""></p>
<hr>
<h3 id="2-配置分词策略"><a href="#2-配置分词策略" class="headerlink" title="2.配置分词策略"></a>2.配置分词策略</h3><h4 id="1-下载IK分词器"><a href="#1-下载IK分词器" class="headerlink" title="1.下载IK分词器"></a>1.下载IK分词器</h4><h4 id="2-将IK解压到-home-solr下"><a href="#2-将IK解压到-home-solr下" class="headerlink" title="2.将IK解压到/home/solr下"></a>2.将IK解压到/home/solr下</h4><h4 id="3-将IK分词器jar文件拷贝到-home-centos-solr-solr-5-5-5-server-solr-webapp-webapp-WEB-INF-lib中"><a href="#3-将IK分词器jar文件拷贝到-home-centos-solr-solr-5-5-5-server-solr-webapp-webapp-WEB-INF-lib中" class="headerlink" title="3.将IK分词器jar文件拷贝到/home/centos/solr/solr-5.5.5/server/solr-webapp/webapp/WEB-INF/lib中"></a>3.将IK分词器jar文件拷贝到/home/centos/solr/solr-5.5.5/server/solr-webapp/webapp/WEB-INF/lib中</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cp &#x2F;home&#x2F;centos&#x2F;solr&#x2F;IK-Analyzer-2012FF&#x2F;dist&#x2F;IKAnalyzer2012_FF.jar &#x2F;home&#x2F;centos&#x2F;solr&#x2F;solr-5.5.5&#x2F;server&#x2F;solr-webapp&#x2F;webapp&#x2F;WEB-INF&#x2F;lib</span><br></pre></td></tr></table></figure>

<h4 id="4-将以下三个配置文件拷贝到-WEB-INF-classes（没有就创建一个）"><a href="#4-将以下三个配置文件拷贝到-WEB-INF-classes（没有就创建一个）" class="headerlink" title="4.将以下三个配置文件拷贝到/WEB-INF/classes（没有就创建一个）"></a>4.将以下三个配置文件拷贝到/WEB-INF/classes（没有就创建一个）</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">stopword.dic</span><br><span class="line">ext.dic</span><br><span class="line">IKAnalyzer.cfg.xml </span><br><span class="line"></span><br><span class="line">cd &#x2F;home&#x2F;centos&#x2F;solr&#x2F;IK-Analyzer-2012FF&#x2F;src</span><br><span class="line">cp stopword.dic ext.dic IKAnalyzer.cfg.xml &#x2F;home&#x2F;centos&#x2F;solr&#x2F;solr-5.5.5&#x2F;server&#x2F;solr-webapp&#x2F;webapp&#x2F;WEB-INF&#x2F;classes</span><br></pre></td></tr></table></figure>

<h4 id="5-在-home-centos-solr-solr-5-5-5-server-solr-new-core-conf下的managed-schema中配置分词器类型"><a href="#5-在-home-centos-solr-solr-5-5-5-server-solr-new-core-conf下的managed-schema中配置分词器类型" class="headerlink" title="5.在/home/centos/solr/solr-5.5.5/server/solr/new_core/conf下的managed-schema中配置分词器类型"></a>5.在/home/centos/solr/solr-5.5.5/server/solr/new_core/conf下的managed-schema中配置分词器类型</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;!-- 配置分词器fieldType--&gt;</span><br><span class="line">&lt;fieldType name&#x3D;&quot;text_ik&quot; class&#x3D;&quot;solr.TextField&quot;&gt;</span><br><span class="line">    &lt;!--索引时候的分词器--&gt;</span><br><span class="line">    &lt;analyzer type&#x3D;&quot;index&quot; isMaxWordLength&#x3D;&quot;false&quot; class&#x3D;&quot;org.wltea.analyzer.lucene.IKAnalyzer&quot;&#x2F;&gt;</span><br><span class="line">    &lt;!--查询时候的分词器--&gt;</span><br><span class="line">    &lt;analyzer type&#x3D;&quot;query&quot; isMaxWordLength&#x3D;&quot;true&quot; class&#x3D;&quot;org.wltea.analyzer.lucene.IKAnalyzer&quot;&#x2F;&gt;</span><br><span class="line">&lt;&#x2F;fieldType&gt;</span><br><span class="line"></span><br><span class="line">&lt;!--将type改为分词器类型--&gt;</span><br><span class="line">&lt;field name&#x3D;&quot;title&quot; type&#x3D;&quot;text_ik&quot; indexed&#x3D;&quot;true&quot; stored&#x3D;&quot;true&quot;&#x2F;&gt;</span><br><span class="line">&lt;field name&#x3D;&quot;author&quot; type&#x3D;&quot;text_ik&quot; indexed&#x3D;&quot;true&quot; stored&#x3D;&quot;true&quot;&#x2F;&gt;</span><br></pre></td></tr></table></figure>
<h4 id="6-测试分词结果"><a href="#6-测试分词结果" class="headerlink" title="6.测试分词结果"></a>6.测试分词结果</h4><p><img src="pic.owlhy.com/blog/20200602225621.png" alt=""></p>
<h4 id="7-添加一个文档到索引"><a href="#7-添加一个文档到索引" class="headerlink" title="7.添加一个文档到索引"></a>7.添加一个文档到索引</h4><p><img src="pic.owlhy.com/blog/20200602225633.png" alt=""></p>
<h4 id="8-查询该文档"><a href="#8-查询该文档" class="headerlink" title="8.查询该文档"></a>8.查询该文档</h4><p><img src="pic.owlhy.com/blog/20200602225659.png" alt=""></p>
<h3 id="注：-1"><a href="#注：-1" class="headerlink" title="注："></a>注：</h3><p>使用分词器分词时可能会报错，原因是solr版本与IK分词器等jar包版本冲突，要注意匹配版本！！！</p>
<h4 id="快捷地址："><a href="#快捷地址：" class="headerlink" title="快捷地址："></a>快捷地址：</h4><p>/home/centos/solr/solr-5.5.5/server/solr-webapp/webapp/WEB-INF/classes</p>
<p>/home/centos/solr/solr-5.5.5/server/solr-webapp/webapp/WEB-INF/lib</p>
<p>/home/centos/solr/solr-5.5.5/server/solr</p>
<p>/home/centos/solr/solr-5.5.5/bin</p>
<h4 id="快捷命令："><a href="#快捷命令：" class="headerlink" title="快捷命令："></a>快捷命令：</h4><p><strong>启动solr:</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">.&#x2F;solr start -p 8080</span><br></pre></td></tr></table></figure>
<p><strong>查看日志:</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">cd &#x2F;home&#x2F;centos&#x2F;solr&#x2F;solr-5.5.5&#x2F;server&#x2F;logs</span><br><span class="line">tail -f solr.log</span><br></pre></td></tr></table></figure>

<p>URL：<a href="http://58.87.90.204:8080/solr" target="_blank" rel="noopener">http://58.87.90.204:8080/solr</a></p>
]]></content>
      <categories>
        <category>solr</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>solr</tag>
      </tags>
  </entry>
  <entry>
    <title>【Solr入门--复制域的详细配置】</title>
    <url>/solr-05.html</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p>solr在没设置默认字段的情况下，搜索格式为“q=name:php学习”的形式，将name设置为默认搜索字段后就可以用“q=php学习”的形式进行搜索了。</p>
<h5 id="solr设置默认搜索字段的方法为："><a href="#solr设置默认搜索字段的方法为：" class="headerlink" title="solr设置默认搜索字段的方法为："></a>solr设置默认搜索字段的方法为：</h5><h6 id="1-在schema-xml配置文件中找到text，一般默认情况下，这行配置是被注释的，取消注释，将text改成你要设置字段名。"><a href="#1-在schema-xml配置文件中找到text，一般默认情况下，这行配置是被注释的，取消注释，将text改成你要设置字段名。" class="headerlink" title="1. 在schema.xml配置文件中找到text，一般默认情况下，这行配置是被注释的，取消注释，将text改成你要设置字段名。"></a>1. 在schema.xml配置文件中找到<defaultSearchField>text</defaultSearchField>，一般默认情况下，这行配置是被注释的，取消注释，将text改成你要设置字段名。</h6><a id="more"></a>

<h6 id="2-打开solrconfig-xml，查找-lt-requestHandler-name-”-select”-，将text修改为要设置的默认字段名或复制域的名。"><a href="#2-打开solrconfig-xml，查找-lt-requestHandler-name-”-select”-，将text修改为要设置的默认字段名或复制域的名。" class="headerlink" title="2. 打开solrconfig.xml，查找&lt;requestHandler name=”/select” ，将text修改为要设置的默认字段名或复制域的名。"></a>2. 打开solrconfig.xml，查找&lt;requestHandler name=”/select” ，将text修改为要设置的默认字段名或复制域的名。</h6><h5 id="附件：复制域的使用，复制域中source属性的值一定要在前面的配置中出现"><a href="#附件：复制域的使用，复制域中source属性的值一定要在前面的配置中出现" class="headerlink" title="附件：复制域的使用，复制域中source属性的值一定要在前面的配置中出现"></a>附件：复制域的使用，复制域中source属性的值一定要在前面的配置中出现</h5><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;field name&#x3D;&quot;msg_title&quot; type&#x3D;&quot;text_ik&quot; indexed&#x3D;&quot;true&quot; stored&#x3D;&quot;true&quot; multiValued&#x3D;&quot;false&quot; &#x2F;&gt;</span><br><span class="line">&lt;field name&#x3D;&quot;msg_text&quot; type&#x3D;&quot;text_ik&quot; indexed&#x3D;&quot;true&quot; stored&#x3D;&quot;true&quot; multiValued&#x3D;&quot;false&quot;  &#x2F;&gt;</span><br><span class="line">&lt;field name&#x3D;&quot;msg_recv&quot; type&#x3D;&quot;string&quot; indexed&#x3D;&quot;true&quot; stored&#x3D;&quot;true&quot; multiValued&#x3D;&quot;false&quot;  &#x2F;&gt;</span><br><span class="line">&lt;field name&#x3D;&quot;keywords&quot; type&#x3D;&quot;text_ik&quot; indexed&#x3D;&quot;true&quot; stored&#x3D;&quot;false&quot;  multiValued&#x3D;&quot;true&quot;&#x2F;&gt;</span><br><span class="line">  &lt;copyField source&#x3D;&quot;id&quot; dest&#x3D;&quot;keywords&quot;&#x2F;&gt;</span><br><span class="line">  &lt;copyField source&#x3D;&quot;msg_title&quot; dest&#x3D;&quot;keywords&quot;&#x2F;&gt;</span><br><span class="line">  &lt;copyField source&#x3D;&quot;msg_text&quot; dest&#x3D;&quot;keywords&quot;&#x2F;&gt;</span><br><span class="line">  &lt;copyField source&#x3D;&quot;msg_recv&quot; dest&#x3D;&quot;keywords&quot;&#x2F;&gt;</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>solr</category>
      </categories>
      <tags>
        <tag>linux</tag>
        <tag>solr</tag>
      </tags>
  </entry>
  <entry>
    <title>【いつだって 信じて 可能性 は無限大】</title>
    <url>/planB.html</url>
    <content><![CDATA[<link rel="stylesheet" class="aplayer-secondary-style-marker" href="/assets/css/APlayer.min.css"><script src="/assets/js/APlayer.min.js" class="aplayer-secondary-script-marker"></script><script class="meting-secondary-script-marker" src="/assets/js/Meting.min.js"></script><p><img src="http://pic.owlhy.com/blog/desktop.jpeg" alt="image"></p>
<a id="more"></a>

<h3 id="冲冲冲"><a href="#冲冲冲" class="headerlink" title="冲冲冲"></a>冲冲冲</h3>]]></content>
      <categories>
        <category>cdx</category>
      </categories>
      <tags>
        <tag>vlog</tag>
        <tag>cdx</tag>
      </tags>
  </entry>
</search>
